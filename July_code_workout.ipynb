{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import all libraries and dependencies for data visualization\n",
    "pd.options.display.float_format='{:.4f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "\n",
    "# import all libraries and dependencies for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, kurtosis, skew\n",
    "\n",
    "# Import specific libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "from scipy.special import boxcox1p\n",
    "from patsy import dmatrices\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "%store -r all_data\n",
    "%store -r y_train\n",
    "%store -r cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##El método de más arriba tira error de sintaxis con el feature 2ndFlrSF porque empieza con 2 (parece),así que rename\n",
    "\n",
    "all_data.rename(columns={'2ndFlrSF':'SndFlrSF'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###treatment of skewed features\n",
    "\n",
    "#Seleccionamos los numeric features only (usamos cols porque esos son los feats que quedaron)\n",
    "numeric_features = all_data.loc[:, list(cols)].dtypes[(all_data.dtypes != \"category\") & (all_data.dtypes != \"uint8\")].index\n",
    "\n",
    "# #Calculamos los skewes y los ordenamos...\n",
    "skewed_features = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending =False)\n",
    "\n",
    "#Seleccionamos los que tengan un abs(skew) >0.7\n",
    "skewed_max = pd.DataFrame({\"Skew\": skewed_features})\n",
    "skewed_max  = skewed_max[abs(skewed_max[\"Skew\"]) > 0.7]\n",
    "skewed_max.dropna()\n",
    "\n",
    "#Aplicamos ahora la BoxCox transformation\n",
    "l_opt = {}\n",
    "for f in skewed_max.index:\n",
    "    all_data[f], l_opt[f] = boxcox((all_data[f] + 1))\n",
    "    \n",
    "#Checking the results...\n",
    "\n",
    "skewed_features_2 = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending =False)\n",
    "sk_2 = pd.DataFrame({\"Skew_before\":skewed_features, \"Skew_after\": skewed_features_2})\n",
    "sk_2 = sk_2[abs(sk_2[\"Skew_before\"]) > 0.7].sort_values(by = \"Skew_before\", ascending = False)\n",
    "sk_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Casi todos los features quedaron con un skew aceptable y cercano al normal, aunque los dos primeros\n",
    "todavía se ven flojos. Una análisis útil para ver cómo quedan y si aúns e pueden abordar con ajustes discretos\n",
    "es el análisis gráfico: QQPlot y Density Plot\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EVALUACION GRÁFICA DE LOS FEATURES TRANSFORMADOS\n",
    "\n",
    "\"\"\"Método para plotear los features\"\"\"\n",
    "\n",
    "def QQ_plot(data, feature_name):\n",
    "    #Setting the canvas\n",
    "    fig = plt.figure(figsize = (12,4))\n",
    "    \n",
    "    #Fit the parameters (estimates the parameters of the function)\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "    \n",
    "    #Plots the kernel density plot KD for the feature\n",
    "    fig1 = fig.add_subplot(121) # una fila con 2 plots, empezando con el 1\n",
    "    sns.distplot(data, fit = norm,  hist=True, kde_kws={'bw':0.05})\n",
    "     \n",
    "\n",
    "#     sns.distplot(data , fit = norm)\n",
    "    fig1.set_title(feature_name + \"Distribution (mu = {:.2f}  and sigma = {:.2f})\".format(mu,sigma), loc = \"center\")\n",
    "    fig1.set_xlabel(feature_name)\n",
    "    fig1.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    #Plots de qq-plot\n",
    "    fig2 = fig.add_subplot(122) # segundo plot\n",
    "    res = probplot(data, plot = fig2)\n",
    "    fig2.set_title(feature_name + \" Probability plot (skewness: {:.6f} and kurtosis: {:.6f} )\".format(data.skew(), data.kurt()),\n",
    "                 loc = \"center\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "for f in skewed_max.index:\n",
    "    QQ_plot(all_data[f], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Puede verse que KitchenAbvGr y MiscVal siguen mostrando un alto skewness. Las otras variables muestran mejorías\n",
    "importantes en sus skewness y en general también en sus kurtosis\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ANALIZAR SI ES POSIBLE INCORPORAR POLYNOMIAL FEATURES WITH THE MOST CORRELATED ENIGINEERED FEATURES\n",
    "\"\"\"Para analizar esta posibilidad primero revisamos cómo se comportan los features elegidos \n",
    "en relación con la variable target. Para ello ploteamos\"\"\"\n",
    "\n",
    "##Most correlated ENGINEERED features:  ['ConstructArea', 'TotalPoints', 'LotAreaMultSlope',  'GarageArea_x_Car', \"TotalExtraPoins\"]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1).corr().SalePrice.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ploteamos ahora estos 5 features y vemos su comportamiento.Será lineal??\n",
    "\n",
    "fig = plt.figure(figsize = (20,15))\n",
    "ax  = fig.add_subplot(331)\n",
    "g = sns.regplot(y = \"SalePrice\", x = \"ConstructArea\", data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1), order = 3 )\n",
    "ax  = fig.add_subplot(332)\n",
    "g = sns.regplot(y = \"SalePrice\", x = \"TotalExtraPoints\", data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1), order = 3)\n",
    "ax  = fig.add_subplot(333)\n",
    "g = sns.regplot(y = \"SalePrice\", x = \"GarageArea_x_Car\", data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1), order = 3)\n",
    "ax  = fig.add_subplot(334)\n",
    "g = sns.regplot(y = \"SalePrice\", x = \"LotAreaMultSlope\", data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1), order = 3)\n",
    "ax  = fig.add_subplot(335)\n",
    "g = sns.regplot(y = \"SalePrice\", x = \"TotalPoints\", data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1), order = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Vemos que todos tiene una relación no-lineal con target, lo cual implica que una curva polinomial sería lo más\n",
    "adecuado para describir la relación de la variable con el target (POLYNOMIAL REGRESSION), ya sea cuadrática, cúbica, etc. Esto implica entonces agregar\n",
    "nuevos features que son polinomiales.\n",
    "Más información: https://youtu.be/Qnt2vBRW8Io\n",
    "https://machinelearningmastery.com/polynomial-features-transforms-for-machine-learning/\n",
    "\n",
    "Diseñaremos y analizaremos nuevos polynomial features para elegir los que mejor resultado dan (R2). Se deja fuera TotalExtraPoints ya que contiene features que ya están en TotalPoints\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METODO PARA EVALUAR LOS POLINOMIOS DE COMBINACIONES DE FEATURES\n",
    "\n",
    "def poly(X, y, feat =\"\"):\n",
    "    \"\"\"Recibe X como un feat o varios y genera polinomios, realiza regresiones y evalúa usando R2\"\"\"\n",
    "\n",
    "    #Instanciamos para obtener los cudráticos, cubos, 4 y 5th \n",
    "    quadratic = PolynomialFeatures(degree = 2, interaction_only = False, include_bias= False)\n",
    "    cubic = PolynomialFeatures(degree = 3, interaction_only = False, include_bias= False)\n",
    "    fourth = PolynomialFeatures(degree = 4, interaction_only = False, include_bias= False)\n",
    "    fifth = PolynomialFeatures(degree = 5, interaction_only = False, include_bias= False)\n",
    "\n",
    "    #Creamos ahora los polinomios\n",
    "    X_quad = quadratic.fit_transform(X)\n",
    "    X_cubic= cubic.fit_transform(X)\n",
    "    X_fourth= fourth.fit_transform(X)\n",
    "    X_fifth = fifth.fit_transform(X)\n",
    "\n",
    "\n",
    "    #We make the regression on each polynomial and compute R2\n",
    "\n",
    "    #Linear fit\n",
    "    regr = LinearRegression()\n",
    "    regr = regr.fit(X,y)\n",
    "    y_lin_predict = regr.predict(X)\n",
    "    linear_r2 = r2_score(y, y_lin_predict)\n",
    "    \n",
    "    #Quadratic fit\n",
    "    regr = regr.fit(X_quad,y)\n",
    "    y_quad_predict = regr.predict(X_quad)\n",
    "    quad_r2 = r2_score(y, y_quad_predict)\n",
    "    \n",
    "    #Cubic fit\n",
    "    regr = regr.fit(X_cubic,y)\n",
    "    y_cubic_predict = regr.predict(X_cubic)\n",
    "    cubic_r2 = r2_score(y, y_cubic_predict)\n",
    "    \n",
    "     #Fourth fit\n",
    "    regr = regr.fit(X_fourth,y)\n",
    "    y_fourth_predict = regr.predict(X_fourth)\n",
    "    fourth_r2 = r2_score(y, y_fourth_predict)\n",
    "    \n",
    "     #Fifth fit\n",
    "    regr = regr.fit(X_fifth,y)\n",
    "    y_fifth_predict = regr.predict(X_fifth)\n",
    "    fifth_r2 = r2_score(y, y_fifth_predict)\n",
    "    \n",
    "\n",
    "    print(\"Results:\")\n",
    "    print(\"Linear:\", linear_r2)\n",
    "    print(\"Quadratic:\", quad_r2)\n",
    "    print(\"Cubic:\",cubic_r2 )\n",
    "    print(\"Fourth:\", fourth_r2)\n",
    "    print(\"Fifth:\", fifth_r2)\n",
    "\n",
    "    if len(feat)>0:\n",
    "        fig = plt.figure(figsize=(20,5))\n",
    "        # Plot lowest Polynomials\n",
    "        fig1 = fig.add_subplot(121)\n",
    "        plt.scatter(X[feat], y, label='training points', color='lightgray')\n",
    "        plt.plot(X[feat], y_lin_predict, label='linear (d=1), $R^2=%.3f$' % linear_r2, color='blue', lw=0.5, linestyle=':')\n",
    "        plt.plot(X[feat], y_quad_predict, label='quadratic (d=2), $R^2=%.3f$' % quad_r2, color='red', lw=0.5, linestyle='-')\n",
    "        plt.plot(X[feat], y_cubic_predict, label='cubic (d=3), $R^2=%.3f$' % cubic_r2,  color='green', lw=0.5, linestyle='--')\n",
    "\n",
    "        plt.xlabel(feat)\n",
    "        plt.ylabel('Sale Price')\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "        # Plot higest Polynomials\n",
    "        fig2 = fig.add_subplot(122)\n",
    "        plt.scatter(X[feat], y, label='training points', color='lightgray')\n",
    "        plt.plot(X[feat], y_lin_predict, label='linear (d=1), $R^2=%.3f$' % linear_r2, color='blue', lw=2, linestyle=':')\n",
    "        plt.plot(X[feat], y_fifth_predict, label='Fifth (d=5), $R^2=%.3f$' % fifth_r2, color='yellow', lw=2, linestyle='-')\n",
    "        plt.plot(X[feat], y_fifth_predict, label='Fourth (d=4), $R^2=%.3f$' % fourth_r2, color='red', lw=2, linestyle=':')\n",
    "\n",
    "        plt.xlabel(feat)\n",
    "        plt.ylabel('Sale Price')\n",
    "        plt.legend(loc='upper left')\n",
    "    else:\n",
    "        # Plot initialisation\n",
    "        fig = plt.figure(figsize=(20,10))\n",
    "        ax = fig.add_subplot(121, projection='3d')\n",
    "        ax.scatter(X.iloc[:, 0], X.iloc[:, 1], y, s=40)\n",
    "\n",
    "        # make lines of the regressors:\n",
    "        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_lin_predict, label='linear (d=1), $R^2=%.3f$' % linear_r2, \n",
    "                 color='blue', lw=2, linestyle=':')\n",
    "        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_quad_predict, label='quadratic (d=2), $R^2=%.3f$' % quad_r2, \n",
    "                 color='red', lw=0.5, linestyle='-')\n",
    "        plt.plot(X.iloc[:, 0], X.iloc[:, 1], y_cubic_predict, label='cubic (d=3), $R^2=%.3f$' % cubic_r2, \n",
    "                 color='green', lw=0.5, linestyle='--')\n",
    "        # label the axes\n",
    "        ax.set_xlabel(X.columns[0])\n",
    "        ax.set_ylabel(X.columns[1])\n",
    "        ax.set_zlabel('Sales Price')\n",
    "        ax.set_title(\"Poly up to 3 degree\")\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Para facilitar el ploteo usamos dataset con saleprice\n",
    "data = pd.concat((all_data.iloc[:len(y_train), :], y_train), axis = 1)\n",
    "X = data.loc[:, [\"ConstructArea\"]]\n",
    "\n",
    "poly(X, y_train, \"ConstructArea\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, ['ConstructArea', 'TotalPoints']]\n",
    "poly(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X= data.loc[:, ['ConstructArea', 'TotalPoints', 'LotAreaMultSlope',  'GarageArea_x_Car']]\n",
    "poly(X, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Se selecciona el último mix en tercer grado dado que da el mayor R2 (a pesar de que no es tan significativo lo que se gana respecto de\n",
    "#la combinación lineal de los 4). Se selecciona el tercer grado debido a que la literatura indica que un mayor grado tiende a overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREACION DE POLYNOMIAL FEATURES Y AGREGACIÓN AL DATASET\n",
    "\n",
    "\"\"\"Generamos ahora los polynomial features en grado 3 con el mix de los 4 features seleccionados\n",
    "y los agregamos al dataset\"\"\"\n",
    "\n",
    "poly_cols = ['ConstructArea', 'TotalPoints', 'LotAreaMultSlope',  'GarageArea_x_Car']\n",
    "pf = PolynomialFeatures(degree= 3, interaction_only= False, include_bias= False) #instaciamos\n",
    "res= pf.fit_transform(all_data.loc[:, poly_cols]) #Se crearon 30 nuevos features a partir de los 4 del mix\n",
    "\n",
    "#Ahora corresponde darles nombres y acoplarlos al dataset\n",
    "names = pf.get_feature_names(poly_cols) #genera una lista con los nombres de los features creados\n",
    "\n",
    "#Nos deshacemos de los espacios en blanco que aparecen en los nombres y los reemplazamos por _ \n",
    "target_feature_names = [f.replace(\" \", \"_\") for f in names]\n",
    "\n",
    "#Creo dataframe con los nuevos feats y sus nombres\n",
    "new_poly_features = pd.DataFrame(data = res, columns= target_feature_names, index = all_data.index)\n",
    "\n",
    "#Eliminamos los features madres para que no se repitan en el dataset\n",
    "\n",
    "new_poly_features.drop(poly_cols, axis = 1, inplace = True)\n",
    "\n",
    "#Acople final\n",
    "\n",
    "all_data = pd.concat([all_data, new_poly_features], axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####NEW SHAPE OF DATASET AFTER POLYNOMIAL FEATURE CREATION\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
