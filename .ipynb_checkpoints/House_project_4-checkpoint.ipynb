{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#SEconde version of notebookincorporating new modules and FE techniques\n",
    "#url: https://www.kaggle.com/mgmarques/houses-prices-complete-solution#House-Prices---Kaggle-Copetitions\n",
    "\n",
    "\n",
    "### LINK: PARA REVISAR TODAS LAS TECNICAS DE DATA PROCESSING: https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114 \n",
    "### REFERENCE KERNELS: https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n",
    "##reference kernl updated: https://www.kaggle.com/amiiiney/price-prediction-top-15-regularization-stacking/comments#2--Data-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "##Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import all libraries and dependencies for data visualization\n",
    "pd.options.display.float_format='{:.4f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "\n",
    "# import all libraries and dependencies for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, kurtosis, skew\n",
    "\n",
    "# Import specific libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Reading and Understanding the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Data/train.csv' does not exist: b'Data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9cb4c9c3816a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Step 1: Load train and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data/test_house_mae.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_file.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Data/train.csv' does not exist: b'Data/train.csv'"
     ]
    }
   ],
   "source": [
    "# READ THE WHOLE DATASET:\n",
    "# We load and read the whole dataset (train + test) because we need to clean and preprocess all of it!!!!\n",
    "#All the procedures must be applied to the entire dataset before split it!\n",
    "\n",
    "#Step 1: Load train and test sets\n",
    "\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "test = pd.read_csv(\"Data/test_house_mae.csv\")\n",
    "test.to_csv(\"test_file.csv\")\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Drop ID from both datasets\n",
    "train.drop(\"Id\", axis = 1, inplace = True)\n",
    "test.drop(\"Id\", axis = 1, inplace = True)\n",
    "\n",
    "# Shape of the datasets\n",
    "train.shape, test.shape, train.shape[0], test.shape[0]\n",
    "\n",
    "#Separar y convertir el target en DataFrame\n",
    "y_train = train[\"SalePrice\"].to_frame()\n",
    "\n",
    "#Step 2: Concatenate both datasets into one\n",
    "\n",
    "df = pd.concat((train,test), sort = False).reset_index(drop = True)\n",
    "\n",
    "#drop target and id columns\n",
    "df.drop(columns = [\"SalePrice\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at dataset head\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modulo: General summary of dataset\n",
    "def summary_2(df, target):\n",
    "    num = df.select_dtypes(exclude= object).columns\n",
    "    cols = [\"Dtype\",\"Uniques\", \"Nulls\",\"% Nulls\", \"Skew\", \"Kurtosis\", \"Correlation\"]\n",
    "    ind = df.columns\n",
    "    dict = {}\n",
    "    dict[\"Dtype\"] = [df[i].dtype for i in ind]\n",
    "    dict[\"Uniques\"] = [df[i].nunique() for i in ind]\n",
    "    dict[\"Nulls\"] =[df[i].isnull().sum().sum() for i in ind]\n",
    "    dict[\"% Nulls\"] = np.round([df[i].isnull().sum().sum()/len(df[i]) for i in ind], decimals = 2)\n",
    "    dict[\"Skew\"] =  np.round(df.skew(), decimals = 3) \n",
    "    dict[\"Kurtosis\"] = np.round(df.kurt(), decimals = 3)\n",
    "    dict[\"Correlation\"] = np.round(df.corr()[target], decimals = 3)\n",
    "    summary = pd.DataFrame(dict, columns = cols, index = ind)\n",
    "    return summary\n",
    "\n",
    "sum = summary_2(train, \"SalePrice\").sort_values(by = \"Dtype\").sort_values(by = \"Correlation\", ascending = False)\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Features with the highest correlations (15)\n",
    "sum.sort_values(by = \"Correlation\", ascending = False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####GENERATE GROUPS OF FEATURES ACCORDING WITH THEIR SKEW/KURT/CORR VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features high skewed right, heavy-tailed distribution, and with high correlation: apply transformations and manage outliers\n",
    "sum_1 = sum[(abs(sum[\"Skew\"]) > 1) & (abs(sum[\"Kurtosis\"]) > 3) & (abs(sum[\"Correlation\"]) > 0.5)][[\"Skew\", \"Kurtosis\", \"Correlation\"]]\n",
    "print(\"Features high skewed right, heavy-tailed distribution, and with high correlation\")\n",
    "sum_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features skewed, heavy-tailed distribution, and with good correlation: apply transformations and manage outliers\n",
    "\n",
    "sum_2 = sum[(abs(sum[\"Skew\"] > 1)) & (abs(sum[\"Kurtosis\"]) > 1) & (abs(sum[\"Correlation\"]) > 0.05)][[\"Skew\", \"Kurtosis\", \"Correlation\"]].drop(sum_1.index)\n",
    "print(\"Features skewed, heavy-tailed distribution, and with high correlation\")\n",
    "sum_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Features high skewed, heavy-tailed distribution, and with low correlation: Maybe we can drop these features, or just use they with other to create a new more important features:\n",
    "\n",
    "sum_3 = sum[(abs(sum[\"Skew\"] > 1)) & (abs(sum[\"Kurtosis\"]) > 1) & (abs(sum[\"Correlation\"]) > 0.01)][[\"Skew\", \"Kurtosis\", \"Correlation\"]].drop((sum_1 + sum_2).index)\n",
    "print(\"Features skewed, heavy-tailed distribution, and with high correlation\")\n",
    "sum_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Chequear como estan distribuidos los features con referencia al target y detectar posibles outliers (mirar primero los con mayor correlacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##OveralQuall\n",
    "fig = plt.figure(figsize=(25,5))\n",
    "ax = fig.add_subplot(121)\n",
    "sns.scatterplot(x =train[\"OverallQual\"], y = train.SalePrice, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Primero: detectar puntos de posibles outliers. Aislarlos y ver qué tienen en común:\n",
    "train[(train[\"OverallQual\"] == 10) & (train[\"SalePrice\"] < 200000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ambos puntos tienen bajo precio a pesar de la overalqual de 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Segundo: revisamos su relacion con  las otras variables de alta correlacion para evaluar los outliers que detectamos\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "#  Box plot overallqual/salePrice\n",
    "fig1 = fig.add_subplot(221); sns.boxplot(x='OverallQual', y='SalePrice', data=train[['SalePrice', 'OverallQual']])\n",
    "\n",
    "#  GrLivArea vs SalePrice plot\n",
    "fig2 = fig.add_subplot(222); \n",
    "sns.scatterplot(x = train.GrLivArea, y = train.SalePrice, hue=train.OverallQual, palette= 'Spectral')\n",
    "\n",
    "# GarageCars vs SalePrice plot\n",
    "fig3 = fig.add_subplot(223); \n",
    "sns.scatterplot(x = train.GarageCars, y = train.SalePrice, hue=train.OverallQual, palette= 'Spectral')\n",
    "\n",
    "#  GarageArea vs SalePrice plot\n",
    "fig4 = fig.add_subplot(224); \n",
    "sns.scatterplot(x = train.GarageArea, y = train.SalePrice, hue=train.OverallQual, palette= 'Spectral')\n",
    "\n",
    "# TotalBsmtSF\n",
    "fig5 = plt.figure(figsize=(16, 8))\n",
    "fig6 = fig5.add_subplot(121); \n",
    "sns.scatterplot(y = train.SalePrice , x = train.TotalBsmtSF, hue=train.OverallQual, palette= 'YlOrRd')\n",
    "\n",
    "# 1stFlrSF\n",
    "fig7 = fig5.add_subplot(122); \n",
    "sns.scatterplot(y = train.SalePrice, x = train['1stFlrSF'], hue=train.OverallQual, palette= 'YlOrRd')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Indentificar los puntos potencialmente outliers...\n",
    "#GrLivArea\n",
    "train[(train[\"GrLivArea\"] > 4500) & (train[\"SalePrice\"] < 200000)]\n",
    "#Ambos puntos tienen bajo precio a pesar de tener la calificacion overalqual maxima..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Revisar los otros features\n",
    "train[(train[\"GarageArea\"] > 1400) & (train[\"SalePrice\"] < 200000)]\n",
    "#Coincide 1298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[(train[\"TotalBsmtSF\"] > 6000) & (train[\"SalePrice\"] < 200000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Puedo eliminar ambos puntos entonces por ser outliers...\n",
    "train.drop(index = [1298,523], axis = 0, inplace = True)\n",
    "\n",
    "#Check the GrLivArea without the outliers\n",
    "fig2 = fig.add_subplot(222); \n",
    "sns.scatterplot(x = train.GrLivArea, y = train.SalePrice, hue=train.OverallQual, palette= 'Spectral')\n",
    "plt.title(\"Correlation of GrLivArea without outliers:{:1.2f}\".format(train[\"GrLivArea\"].corr(train[\"SalePrice\"])))\n",
    "\n",
    "##Mejoró la correlacion de GrLivArea al eliminarse los outliers??\n",
    "print(\"Correlation of grlivarea after removing outliers:\",train[\"GrLivArea\"].corr(train[\"SalePrice\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREACION DE FEATURES CON MEJOR CORRELACION (BETTER PREDICTORS:\n",
    "\n",
    "## Creo lista para guardar los nuevos features:\n",
    "new_features = []\n",
    "\n",
    "##Creo lista para guardar los features a eliminar\n",
    "remove_features = []\n",
    "\n",
    "##GrLivArea y TotRmsAbvGrd\n",
    "#Creo dos tipos posibles de combinaciones de estos dos features y analizo la corrrlacion con target resultante\n",
    "\n",
    "t = train[[ 'GrLivArea', 'TotRmsAbvGrd', 'SalePrice']]\n",
    "t['GrLivAreaByRms'] = train.GrLivArea/train.TotRmsAbvGrd\n",
    "t['GrLivArea_x_Rms'] = train.GrLivArea*train.TotRmsAbvGrd\n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "print(t.corr()[\"SalePrice\"]) #No se mejora la correlacion de griv ni de tot (considerando que ahora la correlacion de griv era de 0.74)\n",
    "\n",
    "#Graficar para ver si hay algú  outlier que pueda aparecer de estas combinaciones\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= 'GrLivAreaByRms', y = \"SalePrice\", data = t )\n",
    "fig_ = fig.add_subplot(122)\n",
    "sns.regplot( x= 'GrLivArea_x_Rms', y = \"SalePrice\", data = t )\n",
    "\n",
    "#Aparece otro outlier:\n",
    "\n",
    "t[(t[\"GrLivArea_x_Rms\"] > 40000) & (t[\"SalePrice\"] < 250000)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Eliminamos el outlier\n",
    "train.drop(index = [635], axis = 0, inplace = True)\n",
    "train.shape[0] #tamaño despues de eliminar el outlier\n",
    "\n",
    "\n",
    "#3Conclusion: solo se elimina el outlier pero se mantiene el mismo feature dado que su correlacion no fue superada por ninguna combinacion nueva\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##En orden de los mas correlacionados continuamos con GarageArea y GarageCars\n",
    "#Creo dos tipos posibles de combinaciones de estos dos features y analizo la corrrlacion con target resultante\n",
    "\n",
    "t = train[[ 'GarageArea', 'GarageCars', 'SalePrice']]\n",
    "t['GarA/car'] = t[\"GarageArea\"]/t[\"GarageCars\"]\n",
    "t['GarageArea * GarageCars'] =  t[\"GarageArea\"] * t[\"GarageCars\"]\n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "t.corr()[\"SalePrice\"] #No se mejora la correlacion de griv ni de tot (considerando que ahora la correlacion de griv era de 0.74)\n",
    "\n",
    "\n",
    "##Vemos que Garage area *cars mejor la correlacion de ambos features: 0.68\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar para ver si hay algú  outlier que pueda aparecer de estas combinaciones\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (25,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= 'GarA/car', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"GarA/car\"].corr(t[\"SalePrice\"])))\n",
    "fig_ = fig.add_subplot(122)\n",
    "sns.regplot( x= 'GarageArea * GarageCars', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"GarageArea * GarageCars\"].corr(t[\"SalePrice\"])))\n",
    "\n",
    "#Aparece otro outlier??\n",
    "t[t[\"GarageArea * GarageCars\"] > 3700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se gana correlacion eliminando el outlier?\n",
    "#Si se gana se elimina, si no se mantiene\n",
    "d = t[t[\"GarageArea * GarageCars\"] < 3700]\n",
    "d[\"GarageArea * GarageCars\"].corr(d[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como se produce un aumento de 0,68 a 0.70 se eliminan los outliers\n",
    "#Eliminamos los outliers\n",
    "train = train[train[\"GarageArea\"] * train[\"GarageCars\"] < 3700] #con eso elimino todos los outliers que aparecieron (4)\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conclusion: se crea un nuevo feature: 'GarageArea * GarageCars' y se eliminan los dos que lo componen\n",
    "\n",
    "new_features.append('GarageArea * GarageCars')\n",
    "remove_features.append([\"GarageArea\", \"GarageCars\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continuamos con los siguientes, relacionados con basement\n",
    "#Creo dos tipos posibles de combinaciones de estos dos features y analizo la corrrlacion con target resultante\n",
    "\n",
    "t = train[[ 'TotalBsmtSF', '1stFlrSF', 'SalePrice']]\n",
    "t['TotalBsmt/1stFlrSF'] = t[\"TotalBsmtSF\"]/t[\"1stFlrSF\"]\n",
    "t['TotalBsmt * 1stFlrSF'] =  t[\"TotalBsmtSF\"] * t[\"1stFlrSF\"]\n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "t.corr()[\"SalePrice\"] #\n",
    "\n",
    "\n",
    "##Vemos que la multiplicacion genera una mejor correlacion de ambos features: 0.68\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar para ver si hay algú  outlier que pueda aparecer de estas combinaciones\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (25,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= \"TotalBsmt/1stFlrSF\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"TotalBsmtSF\"].corr(t[\"SalePrice\"])))\n",
    "fig_ = fig.add_subplot(122)\n",
    "sns.regplot( x= 'TotalBsmt * 1stFlrSF', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"TotalBsmt * 1stFlrSF\"].corr(t[\"SalePrice\"])))\n",
    "\n",
    "#Aparece otro outlier??\n",
    "t[t[\"TotalBsmt * 1stFlrSF\"] > 10000000]\n",
    "\n",
    "#Se gana correlacion eliminando el outlier?\n",
    "#Si se gana se elimina, si no se mantiene\n",
    "d = t[t[\"TotalBsmt * 1stFlrSF\"] < 10000000]\n",
    "d[\"TotalBsmt * 1stFlrSF\"].corr(d[\"SalePrice\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conclusion: no se gana mucha correlacion, de manera que no se elimina el outlier. Se crea un nuevo feature: \"TotalBsmt * 1stFlrSF\" y se eliminan los dos originales\n",
    "new_features.append(\"TotalBsmt * 1stFlrSF\")\n",
    "remove_features.append([\"TotalBsmt\",\"1stFlrSF\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continuamos con YearBuilt y YearRemodAdd\n",
    "#Creo dos tipos posibles de combinaciones de estos dos features y analizo la corrrlacion con target resultante\n",
    "\n",
    "t = train[[ 'YearBuilt', 'YearRemodAdd',\"GarageYrBlt\", 'SalePrice']]\n",
    "t['YearBuilt/YearRemodAdd'] = t[\"YearBuilt\"]/t[\"YearRemodAdd\"]\n",
    "t['YearBuilt * YearRemodAdd'] =  t[\"YearBuilt\"] * t[\"YearRemodAdd\"]\n",
    "t['YearBuilt * GarageYrBlt'] =  t[\"YearBuilt\"] * t[\"GarageYrBlt\"] #También examino esta posibilidad\n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "t.corr()[\"SalePrice\"] #\n",
    "\n",
    "\n",
    "##Vemos que la multiplicacion de los años genera una mejor correlacion de ambos features: 0.5773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar para ver si hay algún  outlier que pueda aparecer de estas combinaciones\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (25,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= \"YearBuilt/YearRemodAdd\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t['YearBuilt/YearRemodAdd'].corr(t[\"SalePrice\"])))\n",
    "fig_ = fig.add_subplot(122)\n",
    "sns.regplot( x= 'YearBuilt * YearRemodAdd', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"YearBuilt * YearRemodAdd\"].corr(t[\"SalePrice\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aparece otro outlier?? revismos los dos de arriba\n",
    "train[(train[\"YearBuilt\"]* train[\"YearRemodAdd\"] > 395000) & (train[\"SalePrice\"] > 700000)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No parecen ser outliers\n",
    "#Conclusion: se agrega \"YearBuilt * \"YearRemodAdd\" como nuevo feature y se eliminan los dos que lo componen\n",
    "new_features.append( \"YearBuilt * YearRemodAdd\")\n",
    "remove_features.append([\"YearBuilt\",\"YearRemodAdd\"])\n",
    "new_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continuamos con FullBath\n",
    "\n",
    "# BsmtFullBath: Basement full bathrooms\n",
    "\n",
    "# BsmtHalfBath: Basement half bathrooms\n",
    "\n",
    "# FullBath: Full bathrooms above grade\n",
    "\n",
    "# HalfBath: Half baths above grade\n",
    "\n",
    "\n",
    "#Creemos y experimentemos con nuevos features\n",
    "t = train[[ 'BsmtFullBath', 'BsmtHalfBath',\"FullBath\", 'HalfBath', \"SalePrice\"]]\n",
    "t['Total_Bath'] = t[\"FullBath\"] + 0.5 * t[\"HalfBath\"] \n",
    "t['Total_Bsmt_Bath'] = t[\"BsmtFullBath\"] + 0.5 * t[\"BsmtHalfBath\"] \n",
    "t[\"TotalBathrooms\"] = t[\"FullBath\"] + 0.5 * t[\"HalfBath\"] + t[\"BsmtFullBath\"] + 0.5 * t[\"BsmtHalfBath\"] \n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "t.corr()[\"SalePrice\"] #\n",
    "\n",
    "\n",
    "##Vemos que  de las dos fórmulas la que da mejor correlacion es Totalbathrooms, con 0.63, lo cual es una mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar para ver si hay algún  outlier que pueda aparecer de estas combinaciones\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (20,12))\n",
    "fig_1 = fig.add_subplot(221)\n",
    "sns.regplot( x= \"Total_Bath\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t['Total_Bath'].corr(t[\"SalePrice\"])))\n",
    "fig_2 = fig.add_subplot(222)\n",
    "sns.regplot( x= 'Total_Bsmt_Bath', y = \"SalePrice\", data = t )\n",
    "fig_3 = fig.add_subplot(223)\n",
    "sns.regplot( x= 'TotalBathrooms', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"TotalBathrooms\"].corr(t[\"SalePrice\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aparece otro outlier?? revismos los dos de arriba\n",
    "t[t[\"TotalBathrooms\"] > 4.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ubico sus indices para ver qué pasa en train\n",
    "train.loc[[738,921],  :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se gana correlacion eliminando los outliers?\n",
    "#Si se gana se elimina, si no se mantiene\n",
    "d = t[t[\"TotalBathrooms\"] < 4.7]\n",
    "d[\"TotalBathrooms\"].corr(d[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#La correlacion mejora. Se eliminan:\n",
    "train = train[(train[\"FullBath\"] + 0.5 * train[\"HalfBath\"] + train[\"BsmtFullBath\"] + 0.5 * train[\"BsmtHalfBath\"] ) < 4.7]\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conclusion: se eliminan los 2 outliers y se incropora el nuevo feature totalbathrooms, eliminando todos los otros:\n",
    "remove_features.append(['BsmtFullBath', 'BsmtHalfBath',\"FullBath\", 'HalfBath'])\n",
    "new_features.append(\"TotalBathrooms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continuamos analizando Porch\n",
    "porch_cols = train.filter(like='orch', axis=1).columns\n",
    "porch_cols #todos estos features aluden a superficie de este tipo de porches que tiene cada casa. Algunos tienen más de uno\n",
    "list(porch_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se les agrega woodeck\n",
    "#Creemos y experimentemos con nuevos features\n",
    "t = train[['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch','WoodDeckSF', \"SalePrice\"]]\n",
    "# t['Total_Bath'] = t[\"FullBath\"] + 0.5 * t[\"HalfBath\"] \n",
    "# t['Total_Bsmt_Bath'] = t[\"BsmtFullBath\"] + 0.5 * t[\"BsmtHalfBath\"] \n",
    "t[\"TotalPorch\"] = t[\"OpenPorchSF\"] + t['EnclosedPorch']+ t['3SsnPorch'] + t['ScreenPorch'] + t['WoodDeckSF'] #sumamos para analizar la superficie total de porch\n",
    "\n",
    "#Analizo la correlacion resultante\n",
    "t.corr()[\"SalePrice\"] #\n",
    "\n",
    "\n",
    "##Vemos que  de las dos fórmulas la que da mejor correlacion es Total, con 0.39, lo cual es una mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graficar para ver si hay algún  outlier que pueda aparecer de estas combinaciones\n",
    "#t[\"TotalPorch\"] = np.log1p(t[\"TotalPorch\"])\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (20,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= 'TotalPorch', y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"TotalPorch\"].corr(t[\"SalePrice\"])))\n",
    "\n",
    "def plot_vars(cols, t, target):\n",
    "    sns.reset_defaults()\n",
    "    sns.set(style=\"ticks\", color_codes=True)\n",
    "    fig = plt.figure(figsize = (10,20))\n",
    "    i = 1\n",
    "    for col in cols:\n",
    "        fig_i= fig.add_subplot(420 + i)\n",
    "        sns.regplot( x= col, y = target, data = t )\n",
    "        plt.title(\"Correlation to target:{:1.4f}\".format(t[col].corr(t[target])))\n",
    "        i +=1\n",
    "        \n",
    "cols = ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch','WoodDeckSF',\"TotalPorch\", \"SalePrice\"]\n",
    "plot_vars(cols, t, \"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion: TotalPorch tiene una mayor correlacion, por lo que se agrega. Sin embargo hay que tenr cuidado porque se ve graficamente que tiene una alta varianza\n",
    "##lo que puede llevar a complex models or overfitting...\n",
    "\n",
    "new_features.append(\"TotalPorch\")\n",
    "remove_features += ['OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch','WoodDeckSF']\n",
    "\n",
    "\n",
    "###PAUSA PARA LEER AQUI EL WILEY Y ENTRENARSE EN LEER MEJOR SCATTERPLOTS E INDETIFICAR COMPORTAMIENTOS (VARIANZAS, ETC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuamos\n",
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###A continuacion se analizan los features relacionados con las dimensiones y similares\n",
    "\n",
    "# LotArea: Lot size in square feet\n",
    "    \n",
    "# LotShape: General shape of property\n",
    "\n",
    "#        Reg\tRegular\t\n",
    "#        IR1\tSlightly irregular\n",
    "#        IR2\tModerately Irregular\n",
    "#        IR3\tIrregular\n",
    "       \n",
    "# LandContour: Flatness of the property\n",
    "\n",
    "#        Lvl\tNear Flat/Level\t\n",
    "#        Bnk\tBanked - Quick and significant rise from street grade to building\n",
    "#        HLS\tHillside - Significant slope from side to side\n",
    "#        Low\tDepression\n",
    "\n",
    "\t\n",
    "# LotConfig: Lot configuration\n",
    "\n",
    "#        Inside\tInside lot\n",
    "#        Corner\tCorner lot\n",
    "#        CulDSac\tCul-de-sac dead end...\n",
    "#        FR2\tFrontage on 2 sides of property\n",
    "#        FR3\tFrontage on 3 sides of property\n",
    "\t\n",
    "# LandSlope: Slope of property\n",
    "\t\t\n",
    "#        Gtl\tGentle slope\n",
    "#        Mod\tModerate Slope\t\n",
    "#        Sev\tSevere Slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cont = [ \"LandContour\", \"LotConfig\"]\n",
    "t = train[[\"LotArea\", \"LotShape\", \"LandContour\", \"LotConfig\", \"LandSlope\", \"SalePrice\"]]\n",
    "\n",
    "def ordinal_encode(data):\n",
    "    oe = OrdinalEncoder()\n",
    "    oe.fit(data)\n",
    "    data_ordinal = oe.transform(data)\n",
    "    return data_ordinal\n",
    "\n",
    "# Se crea una ecoding especial para landslope y lotshape\n",
    "landslope = {}\n",
    "landslope[\"Gtl\"] = 3\n",
    "landslope[\"Mod\"] = 2\n",
    "landslope[\"Sev\"] = 1\n",
    "\n",
    "lotshape = {}\n",
    "\n",
    "lotshape[\"Reg\"] = 4\n",
    "lotshape[\"IR1\"] = 3\n",
    "lotshape[\"IR2\"] = 2\n",
    "lotshape[\"IR3\"] = 1\n",
    "\n",
    "\n",
    "t[\"LotShape\"] = t[\"LotShape\"].map(lotshape)\n",
    "t[\"LandSlope\"] = t[\"LandSlope\"].map(landslope)\n",
    "\n",
    "\n",
    "t[cols_cont] = ordinal_encode(t[cols_cont])\n",
    "t.head(2)\n",
    "t.corr()[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos intentos de new features\n",
    "t[\"Area*Shape*Slope\"] = t[\"LotArea\"] * t[\"LotShape\"] * t[\"LandSlope\"]\n",
    "t[\"Area*Slope\"] = t[\"LotArea\"] * t[\"LandSlope\"]\n",
    "\n",
    "#testing correlations...\n",
    "t.corr()[\"SalePrice\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vars(cols  = [\"LotArea\", \"LotShape\", \"LandContour\", \"LotConfig\", \"LandSlope\", \"Area*Shape*Slope\", \"Area*Slope\"], t = t, target = \"SalePrice\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se ve que Area*Slope genera una correlacion mas alta que la de LotArea sola (.397)\n",
    "\n",
    "#Veamos si hay outliers\n",
    "#Se supone que hay 3, voy a mirar de cerca...\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (20,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= \"Area*Slope\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"Area*Slope\"].corr(t[\"SalePrice\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se ven 3 que estaban ya en el plot de LotArea\n",
    "\n",
    "#Mejora la correlacion si los elimino?\n",
    "\n",
    "d = t[t[\"Area*Slope\"] < 155000]\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (20,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.regplot( x= \"Area*Slope\", y = \"SalePrice\", data = d )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(d[\"Area*Slope\"].corr(t[\"SalePrice\"])))\n",
    "   \n",
    "#La correlacion aumenta a .4106\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion:\n",
    "#Se agrega nuevo feature y se eliminan los dos que lo conforman\n",
    "#Se eliminan los outliers que aparecieron\n",
    "new_features.append(\"Area*Slope\")\n",
    "remove_features += [\"LotArea\",\"LandSlope\"]\n",
    "\n",
    "train = train[train[\"LotArea\"] < 155000]\n",
    "train.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Continuamos analizando neighborhood para ver cuanto influye en el precio\n",
    "#Como es una var categiorica hay que analizarla de otra forma\n",
    "\n",
    "#Precios promedio por vecindario\n",
    "#Cantifdad de casa vendidas por vecindario\n",
    "\n",
    "av_prices = train.groupby(\"Neighborhood\")[\"SalePrice\"].mean().sort_values(ascending = False)\n",
    "av_prices.plot.bar(title = \"Average SalePrice by Neighborhood\", figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sold_neigh = train.groupby(\"Neighborhood\").size().sort_values(ascending = False)\n",
    "sold_neigh.plot.bar(title = \"Nr of houses sold by Neighborhood\", figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Neighborhood\"].value_counts()/train[\"Neighborhood\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.reset_defaults()\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "fig = plt.figure(figsize = (50,5))\n",
    "fig_1 = fig.add_subplot(121)\n",
    "sns.boxplot(x = \"Neighborhood\", y = \"SalePrice\", data = train, hue = \"OverallQual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (23,5))\n",
    "sns.scatterplot(x = \"Neighborhood\", y = \"SalePrice\", data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Relacion entre el año de venta y el precio de venta segun vecindario...\n",
    "neigh = list(train[\"Neighborhood\"].value_counts().index)\n",
    "\n",
    "##Plotear precio promedio por año segun cada vecindario\n",
    "c = train[train[\"Neighborhood\"]== \"Veenker\"].groupby(\"YearBuilt\")[\"SalePrice\"].mean()\n",
    "d = pd.DataFrame({\"YrSold\": c.index, \"Average Price\": c.values} )\n",
    "fig = plt.figure(figsize = (5,3))\n",
    "\n",
    "# ]g = sns.catplot(y= 'YearlyPriceByNeighborhood', x = 'YrSold', col='Neighborhood', data=YearlyPrice, kind=\"point\", aspect=.6, col_wrap=7, height=4, col_order=Neig.index)\n",
    "sns.lineplot(y= 'Average Price', x = 'YrSold', data = d)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Relacion entre el año de venta y el precio de venta segun vecindario...\n",
    "#Tiene más sentido estudiar el valor del m2 por vecindario, como se suele hacer en corretajes\n",
    "\n",
    "neigh = list(train[\"Neighborhood\"].value_counts().index) #lista de barrios\n",
    "\n",
    "t = train[[\"Neighborhood\", \"YrSold\", \"SalePrice\"]]\n",
    "\n",
    "#Creamos un nuevo feature de precio promedio por superficie\n",
    "t[\"TotalArea\"] = (train.TotalBsmtSF.fillna(0) + train.WoodDeckSF.fillna(0) + train.GrLivArea.fillna(0) + \n",
    "                   train.LotArea.fillna(0) + train.MasVnrArea.fillna(0) + train.GarageArea.fillna(0) + \n",
    "                   train.OpenPorchSF.fillna(0) + train[\"3SsnPorch\"].fillna(0) + train.ScreenPorch.fillna(0) + \n",
    "                   train.EnclosedPorch.fillna(0) + train.PoolArea.fillna(0) )\n",
    "\n",
    "t[\"PriceArea\"] = t[\"SalePrice\"] / t[\"TotalArea\"] #precio por m2\n",
    "\n",
    "##Plotear precio promedio por año segun cada vecindario\n",
    "#g = sns.catplot(y= 'YearlyPriceByNeighborhood', x = 'YrSold', col='Neighborhood', data=YearlyPrice, kind=\"point\", aspect=.6, col_wrap=7, height=4, col_order=neigh)\n",
    "# ]g = sns.catplot(y= 'YearlyPriceByNeighborhood', x = 'YrSold', col='Neighborhood', data=YearlyPrice, kind=\"point\", aspect=.6, col_wrap=7, height=4, col_order=Neig.index)\n",
    "\n",
    "\n",
    "# ##Plotear precio promedio por año segun cada vecindario\n",
    "for barrio in neigh:\n",
    "    c = t[t[\"Neighborhood\"]== barrio].groupby(\"YrSold\")[\"PriceArea\"].mean()\n",
    "    d = pd.DataFrame({\"YrSold\": c.index, \"Average Price by sqft\": c.values} )\n",
    "    #fig = plt.figure(figsize = (2,2))\n",
    "    sns.catplot(y= 'Average Price by sqft', x = 'YrSold', data = d,  kind=\"point\", aspect = 1.2, height = 3)\n",
    "    plt.title(\"Average Price by Year Neighborhood {}\".format(barrio))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Lo mismo pero por mes\n",
    "##Relacion entre el año de venta y el precio de venta segun vecindario...\n",
    "#Tiene más sentido estudiar el valor del m2 por vecindario, como se suele hacer en corretajes\n",
    "\n",
    "neigh = list(train[\"Neighborhood\"].value_counts().index) #lista de barrios\n",
    "\n",
    "t = train[[\"Neighborhood\", \"MoSold\", \"SalePrice\"]]\n",
    "\n",
    "#Creamos un nuevo feature de precio promedio por superficie\n",
    "t[\"TotalArea\"] = (train.TotalBsmtSF.fillna(0) + train.WoodDeckSF.fillna(0) + train.GrLivArea.fillna(0) + \n",
    "                   train.LotArea.fillna(0) + train.MasVnrArea.fillna(0) + train.GarageArea.fillna(0) + \n",
    "                   train.OpenPorchSF.fillna(0) + train[\"3SsnPorch\"].fillna(0) + train.ScreenPorch.fillna(0) + \n",
    "                   train.EnclosedPorch.fillna(0) + train.PoolArea.fillna(0) )\n",
    "\n",
    "t[\"PriceArea\"] = t[\"SalePrice\"] / t[\"TotalArea\"] #precio por m2\n",
    "\n",
    "##Plotear precio promedio por año segun cada vecindario\n",
    "#g = sns.catplot(y= 'YearlyPriceByNeighborhood', x = 'MoSold', col='Neighborhood', data=YearlyPrice, kind=\"point\", aspect=.6, col_wrap=7, height=4, col_order=neigh)\n",
    "# ]g = sns.catplot(y= 'YearlyPriceByNeighborhood', x = 'MoSold', col='Neighborhood', data=YearlyPrice, kind=\"point\", aspect=.6, col_wrap=7, height=4, col_order=Neig.index)\n",
    "\n",
    "\n",
    "# ##Plotear precio promedio por año segun cada vecindario\n",
    "for barrio in neigh:\n",
    "    c = t[t[\"Neighborhood\"]== barrio].groupby(\"MoSold\")[\"PriceArea\"].mean()\n",
    "    d = pd.DataFrame({\"MoSold\": c.index, \"Average Price by sqft\": c.values} )\n",
    "    #fig = plt.figure(figsize = (2,2))\n",
    "    sns.catplot(y= 'Average Price by sqft', x = 'MoSold', data = d,  kind=\"point\", aspect = 1.2, height = 3)\n",
    "    plt.title(\"Average Price by Month Neighborhood {}\".format(barrio))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.filter(like='Y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Hay outliers detectables? En month pueden verse varios posibles, pero se opta por los más evidentes, que aparecen en Crawfor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[(t[\"Neighborhood\"] == \"Crawfor\") & (t[\"PriceArea\"] > 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Se eliminan estos outliers:\n",
    "train.drop(index = [1181, 1405], axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new shape\n",
    "print(\"Shape of train dataset after outliers removal from neighborhoods:\", train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###CHECK THE DISTRIBUTION OF TARGET VARIABLE\n",
    "##Modulo para plotear\n",
    "\n",
    "def QQ_plot(data, target):\n",
    "    #Crea la figura\n",
    "    fig = plt.figure(figsize = (13,4))\n",
    "    #Get the fitted parameters used by the function\n",
    "    (mu, sigma) = norm.fit(data)\n",
    "    #Kernel density plot\n",
    "    fig1 = fig.add_subplot(121)\n",
    "    sns.distplot(data, fit = norm)\n",
    "    fig1.set_title(target + ' Distribution ( mu = {:.1f} and sigma = {:.1f} )'.format(mu, sigma), loc='center')\n",
    "    fig1.set_xlabel(target)\n",
    "    fig1.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    #Creamos el QQ-plot\n",
    "    fig2 = fig.add_subplot(122)\n",
    "    res = stats.probplot(data, plot = fig2)\n",
    "    fig2.set_title(target + ' Probability Plot (skewness: {:.2f} and kurtosis: {:.2f} )'.format(data.skew(), data.kurt()), loc='center')\n",
    "    \n",
    "    #Print results\n",
    "    \n",
    "    \n",
    "    \n",
    "QQ_plot(data = train[\"SalePrice\"], target = \"SalePrice\")\n",
    "QQ_plot(data = np.log1p(train[\"SalePrice\"]), target = \"SalePrice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.filter(like = \"Porch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSIS AND EVALUATION OF A NEW POSSIBLE FEATURE ACCORDING TO SOME EXPERT ADVICE\n",
    "\n",
    "\n",
    "#Creamos un nuevo feature de SUPERFICIE CONSTRUIDA\n",
    "\n",
    "\n",
    "\n",
    "t = train[['SalePrice', 'GrLivArea']]\n",
    "t['ConstructArea'] = (train.TotalBsmtSF.fillna(0) + train.WoodDeckSF.fillna(0) + train.GrLivArea.fillna(0) + \n",
    "                       train.MasVnrArea.fillna(0) + train.GarageArea.fillna(0) + train.OpenPorchSF.fillna(0) + \n",
    "                       train[\"3SsnPorch\"].fillna(0) + train.ScreenPorch.fillna(0) + train.EnclosedPorch.fillna(0) + \n",
    "                       train.PoolArea.fillna(0) )\n",
    "\n",
    "#Vale la pena este nuevo feature? Veamos su correlacion con SalePrice\n",
    "fig = plt.figure(figsize = (15,5))\n",
    "fig1 = fig.add_subplot(121)\n",
    "sns.regplot( x= \"GrLivArea\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"GrLivArea\"].fillna(0).corr(t[\"SalePrice\"])))\n",
    "fig2 = fig.add_subplot(122)\n",
    "sns.regplot( x= \"ConstructArea\", y = \"SalePrice\", data = t )\n",
    "plt.title(\"Correlation to target:{:1.4f}\".format(t[\"ConstructArea\"].corr(t[\"SalePrice\"])))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Se ve que ConstructArea (feature compuesto de la suma de los features de conrtsuccion) es mucho mas solido en correlacion y varianza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the dataset for gym_module\n",
    "train.to_csv(\"train_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Conviene conservar los dos features o eliminar GrLivArea? Se realiza un analisis basado en una regresion. \n",
    "##Hacemos una regresion de combinaciones de las dos features respecto de SalePrice, e incluimos combinaciones polinomiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos X e y\n",
    "y = np.log1p(t[\"SalePrice\"]) #se transformó el target\n",
    "\n",
    "\n",
    "#Rescaling X\n",
    "scale = RobustScaler()\n",
    "X = scale.fit_transform(t[[\"ConstructArea\", \"GrLivArea\"]])\n",
    "\n",
    "#Creating train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comenzamos...\n",
    "#Modulo para dar resultados\n",
    "def results():\n",
    "    print(\"Coefficients:\", lr.coef_)\n",
    "    print(\"Root Mean Square Error: %.4f\"% np.expm1(np.sqrt(mean_squared_error(y_test, y_pred))))\n",
    "    print(\"R-squared test set: %.4f\"% r2_score(y_test, y_pred))\n",
    "    print('--------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "          \n",
    "#1. Regresion con solo GrLivArea\n",
    "print(\"Regression with GrlivArea:\")\n",
    "\n",
    "lr.fit(x_train[:,1].reshape(-1,1), y_train)\n",
    "y_pred = lr.predict(x_test[:,1].reshape(-1,1))\n",
    "results()\n",
    "\n",
    "#2. Regresion con solo ConstructArea\n",
    "print(\"Regression with ConstructArea:\")\n",
    "\n",
    "lr.fit(x_train[:,0].reshape(-1,1), y_train)\n",
    "y_pred = lr.predict(x_test[:,0].reshape(-1,1))\n",
    "results()\n",
    "\n",
    "#3. Regresion with both features\n",
    "print(\"Regression with both features:\")\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "results()\n",
    "\n",
    "#4. Polynomial regressor of order 3 with ConstructArea (1,a,a2,a3)\n",
    "print(\"Polynomial regressor of order 3 with ConstructArea:\")\n",
    "cubic = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False) #instanciamos\n",
    "X_cubic = cubic.fit_transform(x_train[:,0].reshape(-1,1)) # se crean todos los features nuevos\n",
    "lr.fit(X_cubic, y_train)\n",
    "X_cubic_test = cubic.fit_transform(x_test[:,0].reshape(-1,1))\n",
    "y_pred = lr.predict(X_cubic_test)\n",
    "results()\n",
    "\n",
    "#5. Polynomial regressor of order 3 with both features (1,a,a2,a3, b, ab2, etc)\n",
    "print(\"Polynomial regressor of order 3 with both features:\")\n",
    "cubic = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False) #instanciamos\n",
    "X_cubic = cubic.fit_transform(x_train) # se crean todos los features nuevos\n",
    "lr.fit(X_cubic, y_train)\n",
    "X_cubic_test = cubic.fit_transform(x_test) #nuevos features para test\n",
    "y_pred = lr.predict(X_cubic_test)\n",
    "results()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Los mejores resultados se obtuvieron del regresor polinomial de orden 3 para ConstructArea solitario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####DATA CLEANING\n",
    "#Concat data for cleaning work:\n",
    "all_data = pd.concat((train.drop(\"SalePrice\", axis =1), test), axis = 0)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.isnull().sum().sort_values(ascending = False)\n",
    "\n",
    "nulos = pd.DataFrame(all_data.isnull().sum().sort_values(ascending = False))\n",
    "nulos[nulos.values > 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nul = summary_2(train, \"SalePrice\")[[\"Dtype\",\"Nulls\",  \"% Nulls\"]].sort_values(by = \"% Nulls\", ascending = False)\n",
    "nul = nul[nul[\"Nulls\"] >0]\n",
    "nul[\"Non_null\"] = train.shape[0] - nul[\"Nulls\"]\n",
    "nul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Hay algun feature categorico que esté muy skewed?? Si es asi, no tiene valor preidctivo y se elimina\n",
    "#En este caso utilities\n",
    "\n",
    "cat_feat = all_data.select_dtypes(include = [\"object\"]).columns \n",
    "\n",
    "def cat_skewed(data, feat): ##Busca y entrega categorical cargadas en extremo a una sola clase\n",
    "    skewed = []\n",
    "    for f in feat:\n",
    "        if all_data.groupby(f).size()[0] >= 0.99 * all_data[f].shape[0]: #cargada 99% a una clase\n",
    "            skewed.append((f, all_data.groupby(f).size()[0]))\n",
    "    return skewed\n",
    "\n",
    "cat_skewed(all_data, cat_feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por lo anterior eliminamos el feature Utilities...\n",
    "all_data.drop([\"Utilities\"], axis = 1, inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Analisis de nulls y data quality en features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partimos por Garage\n",
    "all_data.groupby(\"GarageType\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary\n",
    "#Separo todos los feats rel to Garage\n",
    "#Summary de nulls\n",
    "#Summary de garagearea y garagecars == 0\n",
    "#Sumary de records con garageyear nulo pero garagetype no nulo---> detached\n",
    "\n",
    "#Fiilna de nulos de garagetype con NA (en vez de cero por ser categoricos)\n",
    "\n",
    "#fillna de garagearea con garagetype detached usando median\n",
    "#fillna de garagearea con 0 para el resto\n",
    "\n",
    "#fillna de garagecars con garagetype detached usando median\n",
    "#fillna de garagecars con 0 para el resto\n",
    "\n",
    "#fillna de garageyrblt con garagetype detached usando median\n",
    "#fillna de garageyrbltcon 0 para el resto\n",
    "\n",
    "#fillna de resto que son categ con garagetype detached\n",
    "#fillna de resto categorico con NA para el resto\n",
    "\n",
    "#ANALYSIS\n",
    "\n",
    "## Lo central es ver si hay binconsistencias donde porejemplo no haya año de construccion del garage pero\n",
    "##si hay calificacion de su tipo \n",
    "\n",
    "##Si se revisa se vera que todos los nulls de estas categorias estan juntos, salvo en dos casos donde\n",
    "##el feat garagetype tiene un valor no nulo (detchd). Por eso se toman esos dos casos ylos nulos se reemplazan por las medianas\n",
    "#y modas de cada columna, y al resto de todos los nulos se reemplazan por 0 o NA segun numerico o categorico\n",
    "#Nota: \"NA\" es una categoria agregada a la clasificacion de cada categorico\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = ['GarageYrBlt', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageArea', 'GarageCars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of nulls..\n",
    "all_data[feat].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sumario de los que tienen 0\n",
    "for f in feat:\n",
    "    print(f, \":\", (all_data[f] == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Car y Area tienen ceros, que es como si tuvieran nulos ahi (ademas de sus nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"GarageCars\"] == 0][feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary de records con garageyrblt nulo pero garagetype no nulo---> detached\n",
    "all_data[(all_data[\"GarageYrBlt\"].isnull()) & (all_data[\"GarageType\"].isnull() == False)][feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"GarageYrBlt\"].isnull()) & (all_data[\"GarageArea\"].isnull() == False)][feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fillna de nulos de garagetype con NA (en vez de cero por ser categoricos)\n",
    "all_data[\"GarageType\"] = all_data[\"GarageType\"].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fil_f(all_data, feat):\n",
    "    \n",
    "    for f in feat:\n",
    "        if all_data[f].dtype == 'object':\n",
    "            c_mode = all_data[f].mode()\n",
    "            all_data.loc[all_data[\"GarageType\"] == \"Detchd\",  f] = all_data.loc[all_data[\"GarageType\"] == \"Detchd\", f].fillna(c_mode)\n",
    "            all_data[f] = all_data[f].fillna(\"NA\")\n",
    "        else:\n",
    "            c_median = all_data[f].median()\n",
    "            all_data.loc[all_data[\"GarageType\"] == \"Detchd\",  f] = all_data.loc[all_data[\"GarageType\"] == \"Detchd\", f].fillna(c_median)\n",
    "            all_data[f] = all_data[f].fillna(0)\n",
    "            \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = fil_f(all_data, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the nulls\n",
    "all_data[feat].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEAR ===AHORA ME QUEDAN ANALYSIS Y REPEAT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########NALAISIS DE MASONRY VENEER\n",
    "##Continuamos \n",
    "mas_cols  = all_data.filter(like='Mas', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en cada una\n",
    "all_data[mas_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en ambas\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Type pero no en Area\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols].shape[0]\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Area pero no en Type\n",
    "all_data[(all_data[\"MasVnrType\"].isnull() == False) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Buscar ceros\n",
    "all_data[all_data[\"MasVnrArea\"] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"].isnull() == False) ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\") ][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"MasVnrArea\"] == 0][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clases en MasVnrType\n",
    "all_data[\"MasVnrType\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay algun record donde masvtype es None pero area tiene valor no cero ni nulo?\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\") ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary\n",
    "#Nulls: 23 typre y 24 masvarea\n",
    "##En todas salvo una están ambas nula y nula\n",
    "##Nulo en type pero no en area= 1\n",
    "# Nulo en area pero no entype = 0\n",
    "# Registros con masvnrarea = 0  1738. En todas menos 3 masvrntype es None\n",
    "# Clase de masvnrtype mas frcuente = BrkFace\n",
    "# Hay 7 records donde type es nulo pero area es mayor a cero\n",
    "\n",
    "\n",
    "## Es None una categoria?\n",
    "# Su  no lo es la debo imputar?\n",
    "#O bien None/zero es una forma correcta de de cdecir que no hay amsoneria en la casa?\n",
    "#Que hago con los 1738 registros que dicen tener masvnrtype None pero area cero. Están correctos (ver pregunta anterior)?\n",
    "#Como imputo donde vnrtype es nulo pero tiene area? (mode?)\n",
    "#Como imputo donde vnrty no es nulo pero area si es NUll? (median?)\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: como lo imputo?\n",
    "\n",
    "#Tesis:\n",
    "# - dejar tranquilo None/zeros\n",
    "#Imputar mode/median\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: imputar con el siguiente más frcuente BrkFace\n",
    "\n",
    "##IMPUTACION:\n",
    "\n",
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "# Impouar type nulo con area >0 con Brk face\n",
    "# En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "# Imputar el resto con None y 0 (todos los otros casos)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]\n",
    "\n",
    "#Uso del loc: data.loc[condiciones que deben cunmplir los registros, columna que me interesa] = entre la columna\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\"), \"MasVnrType\"] = \"BrkFace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\"), [\"MasVnrArea\"]] = all_data[(all_data[\"MasVnrType\"] != \"None\") & (all_data[\"MasVnrArea\"] > 0)][\"MasVnrArea\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar el resto con None y 0 (todos los otros casos)\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking final de nulos:\n",
    "all_data[mas_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########NALAISIS DE MASONRY VENEER\n",
    "##Continuamos \n",
    "mas_cols  = all_data.filter(like='Mas', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en cada una\n",
    "all_data[mas_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en ambas\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Type pero no en Area\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols].shape[0]\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Area pero no en Type\n",
    "all_data[(all_data[\"MasVnrType\"].isnull() == False) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Buscar ceros\n",
    "all_data[all_data[\"MasVnrArea\"] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"].isnull() == False) ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\") ][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"MasVnrArea\"] == 0][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clases en MasVnrType\n",
    "all_data[\"MasVnrType\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay algun record donde masvtype es None pero area tiene valor no cero ni nulo?\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\") ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary\n",
    "#Nulls: 23 typre y 24 masvarea\n",
    "##En todas salvo una están ambas nula y nula\n",
    "##Nulo en type pero no en area= 1\n",
    "# Nulo en area pero no entype = 0\n",
    "# Registros con masvnrarea = 0  1738. En todas menos 3 masvrntype es None\n",
    "# Clase de masvnrtype mas frcuente = BrkFace\n",
    "# Hay 7 records donde type es nulo pero area es mayor a cero\n",
    "\n",
    "\n",
    "## Es None una categoria?\n",
    "# Su  no lo es la debo imputar?\n",
    "#O bien None/zero es una forma correcta de de cdecir que no hay amsoneria en la casa?\n",
    "#Que hago con los 1738 registros que dicen tener masvnrtype None pero area cero. Están correctos (ver pregunta anterior)?\n",
    "#Como imputo donde vnrtype es nulo pero tiene area? (mode?)\n",
    "#Como imputo donde vnrty no es nulo pero area si es NUll? (median?)\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: como lo imputo?\n",
    "\n",
    "#Tesis:\n",
    "# - dejar tranquilo None/zeros\n",
    "#Imputar mode/median\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: imputar con el siguiente más frcuente BrkFace\n",
    "\n",
    "##IMPUTACION:\n",
    "\n",
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "# Impouar type nulo con area >0 con Brk face\n",
    "# En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "# Imputar el resto con None y 0 (todos los otros casos)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]\n",
    "\n",
    "#Uso del loc: data.loc[condiciones que deben cunmplir los registros, columna que me interesa] = entre la columna\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\"), \"MasVnrType\"] = \"BrkFace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\"), [\"MasVnrArea\"]] = all_data[(all_data[\"MasVnrType\"] != \"None\") & (all_data[\"MasVnrArea\"] > 0)][\"MasVnrArea\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar el resto con None y 0 (todos los otros casos)\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking final de nulos:\n",
    "all_data[mas_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########NALAISIS DE MASONRY VENEER\n",
    "##Continuamos \n",
    "mas_cols  = all_data.filter(like='Mas', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"GarageYrBlt\"].isnull()) & (all_data[\"GarageCars\"].isnull() == False)][feat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en cada una\n",
    "all_data[mas_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulos en ambas\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull())][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Type pero no en Area\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols].shape[0]\n",
    "all_data[(all_data[\"MasVnrType\"].isnull()) & (all_data[\"MasVnrArea\"].isnull() == False)][mas_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nulo en Area pero no en Type\n",
    "all_data[(all_data[\"MasVnrType\"].isnull() == False) & (all_data[\"MasVnrArea\"].isnull())][mas_cols].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Buscar ceros\n",
    "all_data[all_data[\"MasVnrArea\"] == 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"].isnull() == False) ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\") ][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"MasVnrArea\"] == 0][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clases en MasVnrType\n",
    "all_data[\"MasVnrType\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay algun record donde masvtype es None pero area tiene valor no cero ni nulo?\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\") ].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summary\n",
    "#Nulls: 23 typre y 24 masvarea\n",
    "##En todas salvo una están ambas nula y nula\n",
    "##Nulo en type pero no en area= 1\n",
    "# Nulo en area pero no entype = 0\n",
    "# Registros con masvnrarea = 0  1738. En todas menos 3 masvrntype es None\n",
    "# Clase de masvnrtype mas frcuente = BrkFace\n",
    "# Hay 7 records donde type es nulo pero area es mayor a cero\n",
    "\n",
    "\n",
    "## Es None una categoria?\n",
    "# Su  no lo es la debo imputar?\n",
    "#O bien None/zero es una forma correcta de de cdecir que no hay amsoneria en la casa?\n",
    "#Que hago con los 1738 registros que dicen tener masvnrtype None pero area cero. Están correctos (ver pregunta anterior)?\n",
    "#Como imputo donde vnrtype es nulo pero tiene area? (mode?)\n",
    "#Como imputo donde vnrty no es nulo pero area si es NUll? (median?)\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: como lo imputo?\n",
    "\n",
    "#Tesis:\n",
    "# - dejar tranquilo None/zeros\n",
    "#Imputar mode/median\n",
    "#Hay 7 records donde type es nulo pero area es mayor a cero: imputar con el siguiente más frcuente BrkFace\n",
    "\n",
    "##IMPUTACION:\n",
    "\n",
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "# Impouar type nulo con area >0 con Brk face\n",
    "# En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "# Imputar el resto con None y 0 (todos los otros casos)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar nos None con area >0 (los 7) con el BrkFace (segundo mas frecuente)\n",
    "all_data[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\")][mas_cols]\n",
    "\n",
    "#Uso del loc: data.loc[condiciones que deben cunmplir los registros, columna que me interesa] = entre la columna\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] != 0) & (all_data[\"MasVnrType\"]== \"None\"), \"MasVnrType\"] = \"BrkFace\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #En los 3 casos de masvnrtype distinto de None y area = 0 imputar area con median de area de los type que no son None\n",
    "all_data.loc[(all_data[\"MasVnrArea\"] == 0) & (all_data[\"MasVnrType\"] != \"None\"), [\"MasVnrArea\"]] = all_data[(all_data[\"MasVnrType\"] != \"None\") & (all_data[\"MasVnrArea\"] > 0)][\"MasVnrArea\"].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputar el resto con None y 0 (todos los otros casos)\n",
    "all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\n",
    "all_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Checking final de nulos:\n",
    "all_data[mas_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss = all_data.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss[miss > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CONTINUAMOS CON BASEMENT RELATED FEATURES\n",
    "\n",
    "b_cols  = all_data.filter(like='Bsm', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[b_cols].head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the nulls\n",
    "all_data[b_cols].isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the ones with zeros...\n",
    "b_num = all_data[b_cols].select_dtypes(exclude= object).columns\n",
    "b_cat = all_data[b_cols].select_dtypes(include= object).columns\n",
    "b_num\n",
    "for n in b_num:\n",
    "    print(n, \":\", all_data[all_data[n] == 0].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Notas\n",
    "# casos de feats que tengan clasificacion NA (no tiene basement) pero que igual tenga datos en las otras features\n",
    "# En los casos de cero todos parecen tambien tener cero\n",
    "# No hay ningun feat categorico que tenga clasisfacion NA\n",
    "#Notar que BsmtUnfSF = BsmtFinSF1'  +  'BsmtFinSF2' + 'BsmtUnfSF'\n",
    "#Mirando los nulls, hay 78 registros de casas con fullbsmtsf zero, lo que significa que no tienen bsmt\n",
    "\n",
    "#Acciones\n",
    "# Fiilna de numericos con mediana de sus categorias asociadas: ej BsmtFinSF1 se rellena con valor mediano de su categoria BsmtFinType1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buscando no zeros...\n",
    "\n",
    "def no_zeros(data, col):\n",
    "    nz = pd.DataFrame(columns = col)\n",
    "    for c in col:\n",
    "        pd.concat((nz, data.loc[data[c] == 0, : ]), axis = 0)\n",
    "        \n",
    "    return nz\n",
    "\n",
    "nz2 = no_zeros(all_data, b_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Most frquent classes in each feature\n",
    "for c in b_cat:\n",
    "    print(c, \":\", all_data.groupby(c).size().sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Pares a revisar:\n",
    "\n",
    "#'BsmtFinType1', 'BsmtFinSF1'\n",
    "# 'BsmtFinType2', 'BsmtFinSF2'\n",
    "\n",
    "#Fillna de BsmtFinType1', 'BsmtFinSF1' con moda de BsmtFinType1 (Unf)\n",
    "\n",
    "# fillna de BsmtFinType2', 'BsmtFinSF2' con moda de BsmtFinType2' (Unf)\n",
    "\n",
    "# chequear que pasa con 78 casas de totalbsmt = 0 (no hay subterraneo) \n",
    "# Hay 2 casas que tienen todos los datos menos el BsmtQual (81 rn total cpn 2 que tienen datos y el resto todo pelado)\n",
    "\n",
    "#Ver casos donde Bmstype es no nulo pero tiene  valosres Bsmtsf nulo o cero (se reemplazan con median de su categoria?)\n",
    "\n",
    "#Chequear BsmtCond y los totalbsmtsf\n",
    "\n",
    "#Bsmatexposure: fillna pero solo donde bsmttotal sea >0 (si es cero no hay basement)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3...chequear que pasa con 78 casas de totalbsmt = 0 (no hay subterraneo) \n",
    "\n",
    "all_data[all_data[\"TotalBsmtSF\"] == 0][b_cols] ##estan completamente vacias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revision de los 81 bsmtqual que son nan...\n",
    "all_data[all_data['BsmtQual'].isnull()][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.  Zoom a los registros que no son nulos asociados a sbsmtWQual...\n",
    "all_data[(all_data['BsmtQual'].isnull()) & (all_data[\"TotalBsmtSF\"] != 0) & (all_data[\"TotalBsmtSF\"].isnull() == False)][b_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ver casos donde Bmstype es no nulo pero tiene  valosres Bsmtsf nulo o cero (se reemplazan con median de su categoria?)\n",
    "all_data[(all_data[\"BsmtFinType1\"].isnull == False) & (all_data[\"BsmtFinSF1\"] > 0)][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"BsmtFinType2\"].isnull == False) & (all_data[\"BsmtFinSF2\"] > 0)][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1..Check bsmtcond y Totalbsmsf\n",
    "all_data[all_data[\"BsmtCond\"].isnull() & all_data[\"TotalBsmtSF\"] > 0][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Casos:\n",
    "# basmtcond nulo pero totalbssf no nulo : 1..(hay 3)\n",
    "#BsmtQual nulo pero totalbssf no nulo : ..2 (hay 2)\n",
    "# 78 casas de totalbsmt = 0 (no hay subterraneo) : ..3 (hay 78)\n",
    "#bsmstqual null y bsmtcond no nulo: ..4 ( hay 2)\n",
    "#bsmstqual no null y bsmtcond nulo: ...5 ( hay 3)\n",
    "#bsmtcond o bsmt qual no nulos pero total bsmtsf es cero: No hay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.... bsmstqual null y bsmtcond no nulo\n",
    "all_data[(all_data[\"BsmtQual\"].isnull()) & (all_data[\"BsmtCond\"].isnull() == False)][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5....bsmstqual no null y bsmtcond nulo\n",
    "all_data[(all_data[\"BsmtCond\"].isnull()) & (all_data[\"BsmtQual\"].isnull() == False)][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsmtcond o bsmt qual no nulos pero total bsmtsf es cero\n",
    "all_data[(all_data[\"BsmtCond\"].isnull() == False) & (all_data[\"TotalBsmtSF\"] == 0) ][b_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[(all_data[\"BsmtQual\"].isnull() == False) & (all_data[\"TotalBsmtSF\"] == 0) ][b_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[['BsmtFinType1', 'BsmtFinSF1','BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTA TAREA: CUANDO UNO DE LOS TYPE1 O TYPE2 ESTA BAJO UNF SIGNIFICA UNFINISHED Y POR TANTO DEBE TENER CERO SF\n",
    "# Y LA SUPERICIE SIN CONSTRUIR DEBE IR A BSMTUNFSF\n",
    "\n",
    "##ojo: la moda a veces es \"No\" o no tiene (valor mas repetido), asi que para imputar fijarse en el segundo valor mas repetido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[b_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MISSING DATA IMPUTATION\n",
    "## Bsmatexposure: fillna con moda pero solo donde bsmttotal sea >0 (si es cero no hay basement): Av\n",
    "all_data.loc[(all_data[\"BsmtExposure\"].isnull()) & (all_data[\"TotalBsmtSF\"].isnull() == False) & (all_data[\"TotalBsmtSF\"] > 0) , [\"BsmtExposure\"]] = \"Av\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Idem con BsmtQual\n",
    "all_data.loc[(all_data[\"BsmtQual\"].isnull()) & (all_data[\"TotalBsmtSF\"].isnull() == False) & (all_data[\"TotalBsmtSF\"] > 0) , [\"BsmtQual\"]] = \"TA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Idem con BsmtCond\n",
    "all_data.loc[(all_data[\"BsmtCond\"].isnull()) & (all_data[\"TotalBsmtSF\"].isnull() == False) & (all_data[\"TotalBsmtSF\"] > 0) , [\"BsmtCond\"]] = \"TA\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idem con BsmtFinType2\n",
    "all_data.loc[(all_data[\"BsmtFinType2\"].isnull()) & (all_data[\"BsmtFinSF2\"].isnull() == False) & (all_data[\"BsmtFinSF2\"] > 0) , [\"BsmtFinType2\"]] = \"Unf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"BsmtFinType1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"BsmtFinType1\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idem con BsmtFinType1: NO HAY\n",
    "all_data.loc[(all_data[\"BsmtFinType1\"].isnull()) & (all_data[\"BsmtFinSF1\"].isnull() == False) & (all_data[\"BsmtFinSF1\"] > 0) , [\"BsmtFinType1\"]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sutileza: se encontro un record donde BsmtFinType2 existe pero no tiene area, mientras  que al lado BsmtUnf tiene area.\n",
    "#Por tanto se asume error y hay que apsar ese valor de area de unfinished al area de type2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.loc[(all_data[\"BsmtFinType2\"] == \"BLQ\") & (all_data[\"BsmtFinSF2\"] == 0), :][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moviendo el valor\n",
    "all_data.loc[(all_data[\"BsmtFinType2\"] == \"BLQ\") & (all_data[\"BsmtFinSF2\"] == 0), [\"BsmtFinSF2\"]] = 354.0\n",
    "all_data.loc[(all_data[\"BsmtFinType2\"] == \"BLQ\") & (all_data[\"BsmtFinSF2\"] == 0), [\"BsmtUnfSF\"]] = 0.000000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3Todas las que quedan corresponden a casas sin basement, asi que se fillna cion NA y zero a todo\n",
    "all_data[b_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"BsmtQual\"].isnull()][b_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fillna a categoricos con NA y numericos con 0:\n",
    "\n",
    "for c in b_cat:\n",
    "    all_data[c] = all_data[c].fillna(\"Na\")\n",
    "    \n",
    "for c in b_num:\n",
    "    all_data[c] = all_data[c].fillna(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final check\n",
    "all_data[b_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTINUAMOS CON LOTFRONTAGE\n",
    "\n",
    "all_data[\"LotFrontage\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cols  = all_data.filter(like='Lot', axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lotfrontage es el unico feature con nulls\n",
    "all_data[l_cols].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Que referencia uso para asignar los fillna? La mas razonable seria la mediana de lotfrontage correspondiente al barrio...\n",
    "#3Hay que preocuparse de que no vaya a haber un Nan en neighborhood, pero se chequeo y hay cero null en ese feature\n",
    "all_data.loc[all_data[\"LotFrontage\"].isnull(), \"Neighborhood\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3Filling nans with median according to neighborhood\n",
    "med = []\n",
    "neig = all_data[\"Neighborhood\"].values\n",
    "for n in neig:\n",
    "    med.append(all_data[all_data[\"Neighborhood\"] == n][\"LotFrontage\"].median())\n",
    "    \n",
    "for n, m in zip(neig, med):\n",
    "    all_data.loc[(all_data[\"LotFrontage\"].isnull()) & (all_data[\"Neighborhood\"] == n), [\"LotFrontage\"]] = m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"LotFrontage\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Continuamos con Pool\n",
    "p_col = all_data.filter(like='ool', axis=1).columns\n",
    "p_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[p_col].isnull().sum() #pool area no tiene nulos pero poolqual si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Casos\n",
    "#Poolq nulo y pool area mayor que zero:  fillna con overallqual calificacion/2 como referencia temporal (!)\n",
    "#Poolq nulo y pool area  zero: fillna con \"NA\" No tiene piscina\n",
    "#Pool qc no nulo y poolarea zero: fillna con \"NA\" NO HAY\n",
    "#poolqc no nulo y poolarea nulo: fillna con mediana de pool para esa calificacion: NO HAY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[p_col].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.loc[all_data[\"PoolQC\"].isnull() & all_data[\"PoolArea\"] > 0][\"OverallQual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pool nulo y pool area mayor que zero: fil fillna con overallqual calificacion/2 como referencia temporal\n",
    "all_data[all_data[\"PoolQC\"].isnull() & all_data[\"PoolArea\"] > 0] # un solo caso\n",
    "\n",
    "#Fillna\n",
    "all_data.loc[all_data[\"PoolQC\"].isnull() & all_data[\"PoolArea\"] > 0, [\"PoolQC\"]] = all_data.loc[all_data[\"PoolQC\"].isnull() & all_data[\"PoolArea\"] > 0][\"OverallQual\"]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Poolq nulo y pool area  zero: fillna con \"NA\"\n",
    "all_data.loc[all_data[\"PoolQC\"].isnull() & all_data[\"PoolArea\"] ==  0, [\"PoolQC\"]] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the final nulls\n",
    "all_data[p_col].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking nulls so far...\n",
    "sum = all_data.isnull().sum().sort_values(ascending = False)\n",
    "sum[sum>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuamos con electrical\n",
    "all_data[\"Electrical\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fillna with mode: SBrkr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Electrical\"] = all_data[\"Electrical\"].fillna(\"SBrkr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora kitchen\n",
    "all_data[\"KitchenQual\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[all_data[\"KitchenQual\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].fillna(\"TA\") #mode replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functional\n",
    "all_data[\"Functional\"].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Miscelaneous, alley and fence\n",
    "#Alley is scarce because is incommon to have houses with alleys, so we replace the nulls with NA\n",
    "#Fence (reja): en usa hay muy pocas cass con rejas--->so we replace the nulls with NA\n",
    "##Miscelaneous: no too common ___> so we replace the nulls with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"NA\")\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"NA\")\n",
    "all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FireplaceQ\n",
    "f_col = all_data.filter(like='Fire', axis=1).columns\n",
    "f_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casos\n",
    "#Fireplaces null, fq no null: NO HAY\n",
    "#Fireplaces no null, fq null: fillna with \"NA\" if fireplaces == 0\n",
    "#Fireplaces 0, fq no null\n",
    "#Fireplaces 0, fq null\n",
    "#Fireplaces >0, fq null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fireplaces null, fq no null\n",
    "all_data[(all_data[\"Fireplaces\"].isnull()) & (all_data[\"FireplaceQu\"].isnull() == False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fireplaces no null, fq null\n",
    "all_data[(all_data[\"Fireplaces\"].isnull() == False) & (all_data[\"FireplaceQu\"].isnull())][f_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fireplaces 0, fq no null\n",
    "all_data[(all_data[\"Fireplaces\"] == 0) & (all_data[\"FireplaceQu\"].isnull() == False)][f_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fireplaces 0, fq null: replace with \"NA\"\n",
    "all_data[(all_data[\"Fireplaces\"] == 0) & (all_data[\"FireplaceQu\"].isnull())][f_col]\n",
    "all_data.loc[(all_data[\"Fireplaces\"] == 0) & (all_data[\"FireplaceQu\"].isnull()), [\"FireplaceQu\"]] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"FireplaceQu\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MZoning, exteriors, saletype\n",
    "all_data[\"MSZoning\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(\"RL\")\n",
    "all_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\n",
    "all_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(all_data[\"Exterior1st\"].mode()[0])\n",
    "all_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(all_data[\"Exterior2nd\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"FireplaceQu\"].isnull().sum()\n",
    "\n",
    "#MZoning, exteriors, saletype\n",
    "all_data[\"MSZoning\"].value_counts()\n",
    "\n",
    "all_data[\"MSZoning\"] = all_data[\"MSZoning\"].fillna(\"RL\")\n",
    "all_data[\"SaleType\"] = all_data[\"SaleType\"].fillna(all_data[\"SaleType\"].mode()[0])\n",
    "all_data[\"Exterior1st\"] = all_data[\"Exterior1st\"].fillna(all_data[\"Exterior1st\"].mode()[0])\n",
    "all_data[\"Exterior2nd\"] = all_data[\"Exterior2nd\"].fillna(all_data[\"Exterior2nd\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FINAL CHECK OF NULLS\n",
    "all_data.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correccion error puntual de año erroneo\n",
    "display(all_data.loc[all_data.GarageYrBlt==2207, ['GarageYrBlt', 'YearBuilt']])\n",
    "all_data.loc[all_data.GarageYrBlt==2207.0, 'GarageYrBlt'] = 2007.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FEATURE ENCODING\n",
    "#separa los features\n",
    "f_num = all_data.select_dtypes(exclude= object).columns\n",
    "f_cat = all_data.select_dtypes(include= object).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##O R D I N A L  F E A T U R E  E N C O D I N G\n",
    "\n",
    "#Select ordinal categorical data and nominal also\n",
    "\n",
    "cat_ordinal = [\"LotShape\", 'LandSlope', 'ExterQual', 'ExterCond', \n",
    "       'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "       'BsmtFinType2', 'HeatingQC', \n",
    "       'KitchenQual', 'Functional', 'FireplaceQu',\n",
    "       'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC',\n",
    "       'Fence', \"Street\", 'CentralAir']\n",
    "\n",
    "cat_nominal = list(set(f_cat) - set(cat_ordinal)) ## \"Resta las listas y deja solo los nominales\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para que el encoding tenga realmente sentido hay que darle mas puntaje a las clases excelente, etc y hacia abajo\n",
    "#Por ello solo queda crear diccionarios y mapear cada feature\n",
    "#Para indicar que no hay (ej: basement) se mapea NA con 0\n",
    "\n",
    "\n",
    "all_data[\"LandSlope\"] = all_data[\"LandSlope\"].map({\"Gtl\":3, \"Mod\":2, \"Sev\": 1}) \n",
    "all_data[\"ExterQual\"] = all_data[\"ExterQual\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0})#una escala para varios\n",
    "all_data[\"ExterCond\"] = all_data[\"ExterCond\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"HeatingQC\"] = all_data[\"HeatingQC\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"KitchenQual\"] = all_data[\"KitchenQual\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0})\n",
    "all_data[\"GarageCond\"] = all_data[\"GarageCond\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"PavedDrive\"] = all_data[\"PavedDrive\"].map({\"Y\":3, \"P\":2, \"N\":1}) \n",
    "all_data[\"LotShape\"] = all_data[\"LotShape\"].map({\"Reg\":4, \"IR1\":3, \"IR2\":2, \"IR3\":1}) \n",
    "all_data[\"BsmtQual\"] = all_data[\"BsmtQual\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"BsmtCond\"] = all_data[\"BsmtCond\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"GarageQual\"] = all_data[\"GarageQual\"].map({\"Ex\":5, \"Gd\":4, \"TA\":3, \"Fa\": 2, \"Po\":1, \"NA\": 0}) \n",
    "all_data[\"PoolQC\"] = all_data[\"PoolQC\"].map({\"Ex\":4, \"Gd\":3, \"TA\":2, \"Fa\": 1, \"NA\": 0}) \n",
    "all_data[\"BsmtExposure\"] = all_data[\"BsmtExposure\"].map({\"Gd\":4, \"Av\":3, \"Mn\":2, \"No\":1, \"NA\": 0}) \n",
    "all_data[\"BsmtFinType1\"] = all_data[\"BsmtFinType1\"].map({\"GLQ\":6, \"ALQ\":5, \"BLQ\":4, \"Rec\": 3, \"LwQ\":2, \"Unf\":1,\"NA\":0})\n",
    "all_data[\"BsmtFinType2\"] = all_data[\"BsmtFinType2\"].map({\"GLQ\":6, \"ALQ\":5, \"BLQ\":4, \"Rec\": 3, \"LwQ\":2, \"Unf\":1,\"NA\":0})\n",
    "all_data[\"CentralAir\"] = all_data[\"CentralAir\"].map({\"Y\":1, \"N\": 0}) \n",
    "all_data[\"GarageFinish\"] = all_data[\"GarageFinish\"].map({\"Fin\":3, \"RFn\":2, \"Unf\":1, \"NA\":1}) \n",
    "all_data[\"Functional\"] = all_data[\"Functional\"].map({\"Typ\":7, \"Min1\":6, \"Min2\":5, \"Mod\": 4, \"Maj1\":3, \"Maj2\":2, \"Sev\":1,\"Sal\":0})\n",
    "all_data[\"Street\"] = all_data[\"Street\"].map({\"Grvl\":0, \"Pave\": 1})\n",
    "all_data[\"Fence\"] = all_data[\"Fence\"].map({\"GdPrv\":4,\"MnPrv\":3, \"GdWo\":2, \"MnWw\":1, \"NA\": 0}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"data_5.csv\", na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE ENGINEERING: CREATING NEW FEATURES\n",
    "\n",
    "#1. Including Pool features in Miscfeatures: mescelaneous incoudes tennis court, etc, son it would be correct to do it\n",
    "\n",
    "p_col = all_data.filter(like='ool', axis=1).columns\n",
    "misc_col = all_data.filter(like =\"isc\", axis = 1).columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscamos donde exista una piscina y haya miscfeatures para que la podamos agregar\n",
    "all_data.loc[(all_data[\"MiscVal\"] > 0) & (all_data[\"PoolArea\"] > 0), list(misc_col) + list(p_col)]\n",
    "\n",
    "#En todos los casos en que tengamos PoolArea >0 reemplazar Misfeature por \"Pool\" y a MiscVal sumarle el valor de PoolArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.loc[all_data[\"PoolArea\"] > 0,list(misc_col) + list(p_col)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En todos los casos en que tengamos PoolArea >0 reemplazar Misfeature por \"Pool\" :\n",
    "all_data.loc[all_data[\"PoolArea\"] > 0,[\"MiscFeature\"]] = \"Pool\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.loc[all_data[\"PoolArea\"] > 0,list(misc_col) + list(p_col)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiscVal sumarle el valor de PoolArea\n",
    "all_data.loc[all_data[\"PoolArea\"]>0, [\"MiscVal\"]] = all_data.loc[all_data[\"PoolArea\"]>0, \n",
    "                                                                 [\"MiscVal\", \"PoolArea\"]].\\\n",
    "                                                                    apply(lambda x:(x[\"MiscVal\"] + x[\"PoolArea\"]), axis = 1)\n",
    "                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###CREACION DE NUEVOS FEATURES A PARTIR DE CATEGORICOS ORDINALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Se crea un feature con todos los ordinales, los cuales son relativos a la calidad de la infraestructura\n",
    "all_data[\"TotalPoints\"] = (all_data.ExterQual + all_data.FireplaceQu + all_data.GarageQual + all_data.KitchenQual +\n",
    "                            all_data.BsmtQual + all_data.BsmtExposure + all_data.BsmtFinType1 + all_data.PoolQC + \n",
    "                            all_data.ExterCond + all_data.BsmtCond + all_data.GarageCond + all_data.OverallCond +\n",
    "                            all_data.BsmtFinType2 + all_data.HeatingQC) + all_data.OverallQual**2\n",
    "\n",
    "##Se crea un feature que agrega los puntajes de todos los extras dentro la infarestructura\n",
    "all_data[\"TotalExtraPoints\"] = all_data.HeatingQC + all_data.PoolQC + all_data.FireplaceQu  + all_data.KitchenQual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Qué tan buenos son estos features para el modelo? Se analiza usando la correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.to_csv(\"data_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
