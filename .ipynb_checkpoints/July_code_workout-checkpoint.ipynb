{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import all libraries and dependencies for data visualization\n",
    "pd.options.display.float_format='{:.4f}'.format\n",
    "plt.rcParams['figure.figsize'] = [8,8]\n",
    "pd.set_option('display.max_columns', 350)\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "sns.set(style='darkgrid')\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "\n",
    "# import all libraries and dependencies for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LinearRegression, OrthogonalMatchingPursuit, Lasso, LassoLarsIC, ElasticNet, ElasticNetCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest, RFECV, SelectFromModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, kurtosis, skew\n",
    "\n",
    "# Import specific libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats import diagnostic as diag\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "\n",
    "# Stats\n",
    "from scipy.stats import skew, norm\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, norm, probplot, boxcox\n",
    "from scipy.special import boxcox1p\n",
    "from patsy import dmatrices\n",
    "\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, SelectKBest, RFECV, SelectFromModel\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, OrthogonalMatchingPursuit, Lasso, LassoLarsIC, ElasticNet, ElasticNetCV\n",
    "from sklearn.linear_model import SGDRegressor, PassiveAggressiveRegressor, HuberRegressor, BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor, ExtraTreesRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "import lightgbm as lgb\n",
    "\n",
    "from patsy import dmatrices\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "pd.set_option('expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading datasets\n",
    "%store -r all_data\n",
    "%store -r y_train\n",
    "%store -r cols\n",
    "# %store -r train\n",
    "# %store -r test\n",
    "%store -r df\n",
    "%store -r all_viejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## F E A T U R E   S E L E C T I O N "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SPLITING INTO TRAIN AND TEST SETS FOR THE ANALYSIS\n",
    "\n",
    "\"\"\"Para el proceso de feature selection se utilizará al dataset sin los polynomial features, ya que\n",
    "se quiere analizar los features originales. Recordar que cols con tiene las columnas seleccionadas luego del\n",
    "análisis de correlación y multicollinearity\"\"\"\n",
    "\n",
    "# #Creo el train y se resetea también su index\n",
    "train = all_data.iloc[:len(y_train), :].reset_index(drop = True, inplace = False)\n",
    "train = train.loc[:, cols].reset_index(drop = True, inplace = False)\n",
    "\n",
    "# #Creo el test y se resetea también su index\n",
    "test = all_data.iloc[len(y_train):, :].reset_index(drop = True, inplace = False)\n",
    "test= test.loc[:, cols].reset_index(drop = True, inplace = False)\n",
    "\n",
    "# #Reseteo además el y_train en su index\n",
    "\n",
    "y_train = y_train.reset_index(drop = True, inplace = False)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FEATURES ELECTION USING WRAPPER METHODS\n",
    "\n",
    "##Rescaling: Primero se hace rescaling de los datos usando RobustScaler ya que permite que la tranaformación sea robusta a\n",
    "#outliers\n",
    " \n",
    "scale = RobustScaler()\n",
    "# Data without Polynomials\n",
    "X = pd.DataFrame(scale.fit_transform(train[cols]), columns= cols)\n",
    "\n",
    "#First method: Backward Elimination using p-values\n",
    "\"\"\"Algoritmo: se hace regresion del dataset y se va eliminando progresivamente\n",
    "el feature que muestre el mayor p-value que sea > p_threshold hasta que no haya cambios significativos en \n",
    "los resultados\"\"\"\n",
    "\n",
    "##MODULO MAE: BACKWARD ELIMINATION USING P-VALUES\n",
    "def backwardElimination(X, y, cols, thres): \n",
    "    \"\"\"#input a dataset x,y, threshold and columns of features; output: columns of selected features\"\"\"\n",
    "    #Ajuste y cálculo del p_value maximo\n",
    "    regressor = LinearRegression()\n",
    "    regressor = sm.OLS(y, X[cols]).fit()\n",
    "    max_val = np.max(regressor.pvalues)\n",
    "    \n",
    "    while max_val > thres:\n",
    "        \n",
    "        #Identifico el feature con maximo p_value y actualizo las columnas de features\n",
    "        feat_to_discard = regressor.pvalues[regressor.pvalues.values.astype(float) == max_val].index[0]\n",
    "        cols = cols.drop(feat_to_discard)\n",
    "        \n",
    "        #Vuelvo a ajustar\n",
    "        regressor = LinearRegression()\n",
    "        regressor = sm.OLS(y, X[cols]).fit()\n",
    "        max_val = np.max(regressor.pvalues)\n",
    "       \n",
    "    print(\"We have selected {} features from BackwardElimination\".format(len(cols)))\n",
    "        \n",
    "          \n",
    "    return cols, regressor\n",
    "\n",
    "pv_cols, regressor = backwardElimination(X, y_train, cols, thres = 0.051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_model=sm.OLS(y_train,X)\n",
    "result=ln_model.fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feats, regressor = backwardElimination(X,y_train.values.reshape(-1,1),cols, 0.051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Chequeamos el resultado corriendo  el OLS en X[selected_features]\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor = sm.OLS(y_train.values.reshape(-1,1), X[selected_feats]).fit()\n",
    "print(regressor.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Method: RECURSIVE FEATURE ELIMINIATION - RFE\n",
    "\"\"\"Definition: Utilizando un estimador (modelo) el algoritmo parte con todos los features y va eliminando aquellos\n",
    "que menos contribuyen al óptimo performance del modelo, hasta llegar a los n features estipulados\"\"\"\n",
    "\"\"\"Links: https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15\n",
    "Lasso y Ridge Regression: https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Usaremos RFECV (REF with crossvalidation included), tomando como estimator LASSO Regression\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el estimator: LASSO Regression\n",
    "ls = Lasso(alpha = 0.0005, max_iter = 161, selection = 'cyclic', tol = 0.002, random_state = 101)\n",
    "\n",
    "#Instanciamos RFECV\n",
    "rfecv = RFECV(estimator=ls, n_jobs = -1, step=1, scoring = 'neg_mean_squared_error' ,cv=5)\n",
    "\n",
    "#Ejecutamos el fit\n",
    "rfecv.fit(X,y_train)\n",
    "\n",
    "#Features seleccionados\n",
    "sel_rfecv = rfecv.get_support() #entrega lista con booleanos True/false si fueron o no seleccionados\n",
    "sel_cols_rfecv = cols[sel_rfecv]\n",
    "\n",
    "#Resultados\n",
    "print(\"RFECV selected {} features\".format(len(sel_cols_rfecv)))\n",
    "print(\"Selected Features:\", sel_cols_rfecv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####IMPORTANTE: ALGUNOS METODOS COMO SBS VAN CALCULANDO A MEDIDA QUE SE VAN ENCONTRANDO CON LOS FEATURES\n",
    "##POR TANTO EL ORDEN EN QUE VAN LAS COLUMNAS EN EL DATASET INFLUYE EN LOS FEATURES QUE SELECCIONA.\n",
    "##ME OCURRIO CON SBS, DONDE EL VIEJO TENIA UN DATAFRAME DF QUE TENIA UN ORDEN DIFERENTE DE COLUMNAS\n",
    "##QUE MI X, POR LO QUE DABA UN RESULTADO DISTINTO. HICE ELÑ SIGUIENTE CAMBIO Y SE SOLUCIONO:\n",
    "## X = X[df.columns]\n",
    "##Con eso quedaron todas las columnas en el mismo orden que en df\n",
    "##EN TODO CASO, PARA NO CONFUNDIRMOE EN ESTE NOTEBOOK IMPORTARÉ LAS COLUMNAS DIRECTO DESDE EL VIEJO\n",
    "\n",
    "###CAMBIO DE ORDEN EN COLUMNAS:\n",
    "cols_viejo = ['LotFrontage',\n",
    " 'Street',\n",
    " 'LotShape',\n",
    " 'LandSlope',\n",
    " 'OverallCond',\n",
    " 'YearBuilt',\n",
    " 'YearRemodAdd',\n",
    " 'MasVnrArea',\n",
    " 'ExterQual',\n",
    " 'ExterCond',\n",
    " 'BsmtQual',\n",
    " 'BsmtCond',\n",
    " 'BsmtExposure',\n",
    " 'BsmtFinType1',\n",
    " 'BsmtFinType2',\n",
    " 'HeatingQC',\n",
    " 'CentralAir',\n",
    " 'BsmtFullBath',\n",
    " 'BsmtHalfBath',\n",
    " 'FullBath',\n",
    " 'HalfBath',\n",
    " 'BedroomAbvGr',\n",
    " 'KitchenAbvGr',\n",
    " 'KitchenQual',\n",
    " 'TotRmsAbvGrd',\n",
    " 'Functional',\n",
    " 'Fireplaces',\n",
    " 'GarageYrBlt',\n",
    " 'GarageFinish',\n",
    " 'PavedDrive',\n",
    " 'OpenPorchSF',\n",
    " 'EnclosedPorch',\n",
    " 'Fence',\n",
    " 'MiscVal',\n",
    " 'YrSold',\n",
    " 'TotalExtraPoints',\n",
    " 'TotalPoints',\n",
    " 'GarageArea_x_Car',\n",
    " 'ConstructArea',\n",
    " 'Garage_Newest',\n",
    " 'LotAreaMultSlope',\n",
    " 'TotBathrooms',\n",
    " 'MSSubClass_120',\n",
    " 'MSSubClass_160',\n",
    " 'MSSubClass_180',\n",
    " 'MSSubClass_190',\n",
    " 'MSSubClass_20',\n",
    " 'MSSubClass_30',\n",
    " 'MSSubClass_40',\n",
    " 'MSSubClass_50',\n",
    " 'MSSubClass_60',\n",
    " 'MSSubClass_70',\n",
    " 'MSSubClass_75',\n",
    " 'MSSubClass_80',\n",
    " 'MSSubClass_85',\n",
    " 'MSZoning_FV',\n",
    " 'MSZoning_RH',\n",
    " 'MSZoning_RM',\n",
    " 'Alley_NA',\n",
    " 'Alley_Pave',\n",
    " 'LandContour_HLS',\n",
    " 'LandContour_Low',\n",
    " 'LandContour_Lvl',\n",
    " 'LotConfig_CulDSac',\n",
    " 'LotConfig_FR2',\n",
    " 'LotConfig_FR3',\n",
    " 'LotConfig_Inside',\n",
    " 'Neighborhood_Blmngtn',\n",
    " 'Neighborhood_Blueste',\n",
    " 'Neighborhood_BrDale',\n",
    " 'Neighborhood_BrkSide',\n",
    " 'Neighborhood_ClearCr',\n",
    " 'Neighborhood_CollgCr',\n",
    " 'Neighborhood_Crawfor',\n",
    " 'Neighborhood_Edwards',\n",
    " 'Neighborhood_Gilbert',\n",
    " 'Neighborhood_IDOTRR',\n",
    " 'Neighborhood_MeadowV',\n",
    " 'Neighborhood_Mitchel',\n",
    " 'Neighborhood_NAmes',\n",
    " 'Neighborhood_NPkVill',\n",
    " 'Neighborhood_NoRidge',\n",
    " 'Neighborhood_NridgHt',\n",
    " 'Neighborhood_OldTown',\n",
    " 'Neighborhood_SWISU',\n",
    " 'Neighborhood_Sawyer',\n",
    " 'Neighborhood_SawyerW',\n",
    " 'Neighborhood_Somerst',\n",
    " 'Neighborhood_StoneBr',\n",
    " 'Neighborhood_Timber',\n",
    " 'Neighborhood_Veenker',\n",
    " 'Condition1_Artery',\n",
    " 'Condition1_Feedr',\n",
    " 'Condition1_Norm',\n",
    " 'Condition1_PosA',\n",
    " 'Condition1_RRAe',\n",
    " 'Condition1_RRAn',\n",
    " 'Condition1_RRNe',\n",
    " 'Condition1_RRNn',\n",
    " 'Condition2_Artery',\n",
    " 'Condition2_Feedr',\n",
    " 'Condition2_Norm',\n",
    " 'Condition2_PosA',\n",
    " 'Condition2_PosN',\n",
    " 'BldgType_2fmCon',\n",
    " 'BldgType_Twnhs',\n",
    " 'HouseStyle_15Fin',\n",
    " 'HouseStyle_15Unf',\n",
    " 'HouseStyle_25Unf',\n",
    " 'HouseStyle_SFoyer',\n",
    " 'HouseStyle_SLvl',\n",
    " 'RoofStyle_Gambrel',\n",
    " 'RoofStyle_Hip',\n",
    " 'RoofStyle_Mansard',\n",
    " 'RoofStyle_Shed',\n",
    " 'RoofMatl_CompShg',\n",
    " 'RoofMatl_TarGrv',\n",
    " 'RoofMatl_WdShake',\n",
    " 'RoofMatl_WdShngl',\n",
    " 'Exterior1st_AsbShng',\n",
    " 'Exterior1st_AsphShn',\n",
    " 'Exterior1st_BrkComm',\n",
    " 'Exterior1st_BrkFace',\n",
    " 'Exterior1st_CemntBd',\n",
    " 'Exterior1st_HdBoard',\n",
    " 'Exterior1st_Plywood',\n",
    " 'Exterior1st_Stucco',\n",
    " 'Exterior1st_WdSdng',\n",
    " 'Exterior1st_WdShing',\n",
    " 'Exterior2nd_AsbShng',\n",
    " 'Exterior2nd_AsphShn',\n",
    " 'Exterior2nd_BrkCmn',\n",
    " 'Exterior2nd_BrkFace',\n",
    " 'Exterior2nd_CmentBd',\n",
    " 'Exterior2nd_HdBoard',\n",
    " 'Exterior2nd_ImStucc',\n",
    " 'Exterior2nd_MetalSd',\n",
    " 'Exterior2nd_Plywood',\n",
    " 'Exterior2nd_Stone',\n",
    " 'Exterior2nd_Stucco',\n",
    " 'Exterior2nd_WdSdng',\n",
    " 'Exterior2nd_WdShng',\n",
    " 'MasVnrType_BrkFace',\n",
    " 'MasVnrType_Stone',\n",
    " 'Foundation_BrkTil',\n",
    " 'Foundation_PConc',\n",
    " 'Foundation_Slab',\n",
    " 'Foundation_Stone',\n",
    " 'Foundation_Wood',\n",
    " 'Heating_GasA',\n",
    " 'Heating_GasW',\n",
    " 'Heating_Grav',\n",
    " 'Heating_Wall',\n",
    " 'Electrical_FuseA',\n",
    " 'Electrical_FuseF',\n",
    " 'Electrical_FuseP',\n",
    " 'GarageType_Basment',\n",
    " 'GarageType_BuiltIn',\n",
    " 'GarageType_CarPort',\n",
    " 'GarageType_Detchd',\n",
    " 'GarageType_NA',\n",
    " 'MiscFeature_Othr',\n",
    " 'MiscFeature_Pool',\n",
    " 'MiscFeature_Shed',\n",
    " 'MoSold_1',\n",
    " 'MoSold_11',\n",
    " 'MoSold_12',\n",
    " 'MoSold_2',\n",
    " 'MoSold_3',\n",
    " 'MoSold_4',\n",
    " 'MoSold_5',\n",
    " 'MoSold_6',\n",
    " 'MoSold_7',\n",
    " 'MoSold_8',\n",
    " 'MoSold_9',\n",
    " 'SaleType_CWD',\n",
    " 'SaleType_Con',\n",
    " 'SaleType_ConLD',\n",
    " 'SaleType_ConLI',\n",
    " 'SaleType_ConLw',\n",
    " 'SaleType_Oth',\n",
    " 'SaleType_WD',\n",
    " 'SaleCondition_Abnorml',\n",
    " 'SaleCondition_AdjLand',\n",
    " 'SaleCondition_Alloca',\n",
    " 'SaleCondition_Family',\n",
    " 'Remod',\n",
    " 'IsNew']\n",
    "X = X[cols_viejo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Third Method: SEQUENTIAL FEATURE SELECTION\n",
    "\"\"\"Computationally very expensive, but we will apply it to get selected features to add to all other\n",
    "techniques we will try\"\"\"\n",
    "\n",
    "from itertools import combinations # nos permitirá generar todas las combinaciones posibles de un arreglo o lista\n",
    "\n",
    "#METHOD: Sequential Backward Selection (SBS)\n",
    "class SBS():\n",
    "    def __init__(self, estimator, k_features, scoring = r2_score, test_size = 0.25, random_state = 101):\n",
    "        self.estimator = estimator\n",
    "        self.scoring = scoring\n",
    "        self.k_features = k_features\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        #Fit: método principal. Toma un dataset y entrega la combinacion de features que optimice el rendimiento\n",
    "    def fit(self, X,y):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = self.test_size, random_state = self.random_state)\n",
    "        dim = X_train.shape[1] #nro inicial de features, que es el total\n",
    "#         df_x_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "#         display(df_x_train.head())\n",
    "        self.indices_ = list(range(dim)) #lista con los indices de todos los features iniciales \n",
    "        self.subsets_ = [self.indices_] # Crea una matriz de 1 fila por n columnas de los features inciales\n",
    "       \n",
    "       \n",
    "        #Calcula el score inicial con todos los features\n",
    "        score = self._calc_score(X_train, X_test, y_train, y_test, self.indices_)\n",
    "        self.scores_ = [score] #crea una lista y agrega el primer score\n",
    "        \n",
    "        #Calcula los scores para cada una de las combinaciones posibles de r features por cada nro de features, partiendo por dim -1:\n",
    "        while dim > self.k_features:\n",
    "            scores = []\n",
    "            subsets = []\n",
    "            for p in combinations(self.indices_, r = dim-1): #se parte por dim -1 porque ya arriba se calculó para el total de los features\n",
    "                score = self._calc_score(X_train, X_test, y_train, y_test, list(p))\n",
    "                scores.append(score) #guardo el score que esta combinación sacó\n",
    "                subsets.append(list(p)) #agrego la combinación aquí\n",
    "                               \n",
    "                \n",
    "            #encuentro ahora la combinación con mejor score\n",
    "            best = np.argmax(scores)\n",
    "            self.indices_ = subsets[best] # en self indices queda la mejor combinación\n",
    "            self.subsets_.append(self.indices_) #agrego a subsets esta combinacion como un bloque []\n",
    "            dim -= 1 #actualizo el dim para volver al while\n",
    "            self.scores_.append(scores[best]) #agrego el score actual a scores_\n",
    "\n",
    "        self.k_score = self.scores_[-1] #k_score es el último valor guardado\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X): #retorna el X actualizado usando solo los mejores indices seleccionados\n",
    "        return X.iloc[:, self.indices_]\n",
    "    \n",
    "\n",
    "        \n",
    "    def _calc_score(self, X_train, X_test, y_train, y_test, indices): #calcula el score de cada combinacion de features\n",
    "        self.estimator.fit(X_train.iloc[:, indices], y_train)\n",
    "        y_predict = self.estimator.predict(X_test.iloc[:, indices])\n",
    "        score = self.scoring(y_test, y_predict)\n",
    "        return score\n",
    "    \n",
    "    \n",
    "#Corremos SBS generando primero los parámetros...\n",
    "score = r2_score\n",
    "ls = Lasso(alpha = 0.0005, max_iter = 161, selection = 'cyclic', tol = 0.002, random_state = 101)\n",
    "\n",
    "#Instanciamos SBS para que haga la selección revisando hasta llegar a 1 feature\n",
    "sbs = SBS(estimator = ls , k_features = 1, scoring = score)\n",
    "\n",
    "\n",
    "sbs.fit(X,y_train)\n",
    "\n",
    "##Ploteamos para ver cuál es la combinación y nro de features con mejor desempeño (r2_score)\n",
    "k_features_plot = [len(k) for k in sbs.subsets_] \n",
    "k_scores = [sc for sc in sbs.scores_]\n",
    "\n",
    "plt.title(\"Nr. Features vs Score\")\n",
    "plt.xlabel(\"Nr. of features\")\n",
    "plt.ylabel(\"R2 Score\")\n",
    "plt.plot(k_features_plot, k_scores, marker = \"o\")\n",
    "\n",
    "print(\"Best Score:{}\".format(max(sbs.scores_)))\n",
    "SBS = list(X.columns[list(sbs.subsets_[max(np.arange(0, len(sbs.scores_))[(sbs.scores_==max(sbs.scores_))])])])\n",
    "print(\"Number of best features selected:{}\".format(len(SBS)))\n",
    "\n",
    "print(\"Selected features by SBS:\", SBS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FEATURE SELECTION BY FILTER METHODS###\n",
    "\"\"\"Se trata de métodos utilizados duranmte la etapa de preprocessing, antes de modelar.\n",
    "Según los tipos de variables,s e aplica un test distinto para ver su grado de correlación con target (ANOVA, Chi2, Pearson)\n",
    "y de acuerdo a eso se rankean.\n",
    "These methods are particularly good for gaining a better understanding of data, but not necessarily for optimizing \n",
    "the feature set for better generalization.\"\"\"\n",
    "\"\"\"Más informaxción: https://www.analyticsvidhya.com/blog/2016/12/\n",
    "introduction-to-feature-selection-methods-with-an-\n",
    "example-or-how-to\n",
    "-select-the-right-\n",
    "variables/#:~:text=Filter%20methods%20are%20generally%20used,is%20a%20subjective%20term%20here.\"\"\"\n",
    "\n",
    "###Utilizaremos el método SelectKBest de Sklearn como filter metodo de seleccion de features: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "\n",
    "##METODO SELECTKBEST:\n",
    "#Hay dos modalidades: usando fregression o mutual info regression\n",
    "\n",
    "#Con f_regression como funcion para rankear\n",
    "skb = SelectKBest(score_func = f_regression, k = 80) #seleccionaremos los mejores 80 feats ordenándolos por su F-value\n",
    "skb.fit(X, y_train)\n",
    "select_features_kbest = skb.get_support() #entrega lista con booleanos True/false si fueron o no seleccionados\n",
    "kbest_FR = cols[select_features_kbest] # columnas con los features seleccionados\n",
    "scores = skb.scores_[select_features_kbest] # extrae los scores de los features seleccionados\n",
    "feature_scores = pd.DataFrame([(item, score) for item,score in zip(kbest_FR, scores)], columns = [\"Feature\", \"Score\"])\n",
    "#Ploteamos\n",
    "fig = plt.figure(figsize= (40,20))\n",
    "f1 = fig.add_subplot(121)\n",
    "feature_scores.sort_values(by = \"Score\", ascending = True).plot(y = \"Score\", x = \"Feature\", kind = \"barh\",\n",
    "                                                                 ax = f1, fontsize = 15, grid = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Con mutual_info_regression como funcion para rankear\n",
    "\n",
    "skb = SelectKBest(score_func=mutual_info_regression, k = 80)\n",
    "skb.fit(X, y_train)\n",
    "select_features_kbest = skb.get_support() #lista d booleanos con True si fue seleccionado dentro de los k mejores\n",
    "kbest_MIR = cols[select_features_kbest] #lista con los nombres de los feats seleccionados por este metodo\n",
    "scores = skb.scores_[select_features_kbest] #extrae los scores de los features seleccionado\n",
    "feature_scores = pd.DataFrame([(item, score) for item, score in zip(kbest_MIR, scores)], columns = [\"Feature\", \"Score\"])\n",
    "\n",
    "#Ploteamos\n",
    "fig2 = plt.figure(figsize = (40,20))\n",
    "f2 = fig2.add_subplot(121)\n",
    "feature_scores.sort_values(by = \"Score\", ascending = True).plot(y=\"Score\", x= \"Feature\", kind = \"barh\",\n",
    "                                                               ax = f2, fontsize = 15, grid = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####NOTA: EL VIEJO TIENE UN ERROR AL CALCULAR EL DATAFRAME DEL METODO MIR, YA QUE HACE:\n",
    "##feature_scores = pd.DataFrame([(item, score) for item,score in zip(kbest_FR, scores)], columns = [\"Feature\", \"Score\"])\n",
    "##EN VEZ DE:\n",
    "###pd.DataFrame([(item, score) for item, score in zip(kbest_MIR, scores)], columns = [\"Feature\", \"Score\"])\n",
    "##POR TANTO HAY QUIZA UNA DIFERENCIA MAS ADELANTE EN SUS ELECCION TOTAL DE FEATURES DE TODOS LOS METODOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###FEATURE SELECTION BY EMBEDDED METHODS###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature selection using XGBoost \n",
    "\"\"\"Puede utilizarse el método de Feature Importance de XGBoot para identificar los features con mayor influencia.\n",
    "Toda la información en: \n",
    "https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27\n",
    "https://medium.com/@gabrieltseng/gradient-boosting-and-xgboost-c306c1bcfaf5\n",
    "https://machinelearningmastery.com/feature-importance-and-feature-selection-with-xgboost-in-python/#:~:text=Generally%2C%20importance%20provides%20a%20score,the%20higher%20its%20relative%20importance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "###Módulo para Feature selection using XGBOOST\n",
    "\"\"\"Modulo usa XGBoost para extraer los features mas importantes, iterando con varios umbrales de corte (thresholds)\n",
    "y seleccionando el subset con mejor rendimiento al hacer correr el XGBoost\"\"\"\n",
    "\n",
    "##Split data into train and test sets for training with XGBOOST\n",
    "X_train, X_test, y, y_test = train_test_split(X, y_train, test_size = 0.30, random_state = 101)\n",
    "\n",
    "#Fit then model in all training data\n",
    "\"\"\"There are many types of importance_type factor to setup, but we will use GAIN, which mean we will give importance\n",
    "to each feature according to the reduction of loss as a result of splitting with it (The average training loss reduction\n",
    "gained when using a feature for splitting.)\"\"\"\n",
    "\n",
    "model = XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, max_delta_step=0, \n",
    "                      random_state=101, min_child_weight=1, missing=None, n_jobs=4,  \n",
    "                      scale_pos_weight=1, seed=None, silent=True, subsample=1, verbosity = 0) #importance_type es gain por defecto\n",
    "\n",
    "#Model fit\n",
    "model.fit(X_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploteamos para revisar\n",
    "fig = plt.figure(figsize = (23,35))\n",
    "ax = fig.add_subplot(121)\n",
    "g = plot_importance(model, height = 0.5, ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora iremos probando con todos los valores de importancia como threshold para \n",
    "#ir viendo qué cantidad de features es la óptima (para lo cual primero fit y luego evaluamos usando R2 y MSE)\n",
    "\n",
    "thresholds = np.sort(np.unique(model.feature_importances_)) #tomo todos los valores únicos de importance y los ordeno de menor a mayor\n",
    "best = 1e36\n",
    "colsbest = 31\n",
    "my_model = model\n",
    "threshold = 0\n",
    "\n",
    "#Utilizo el metodo SelectFromModel para seleccionar los features son threshold >= al vigente\n",
    "for thresh in thresholds:\n",
    "    selection = SelectFromModel(model, threshold = thresh, prefit = True)\n",
    "    select_X_train = selection.transform(X_train) ##crea un X con los feats seleccionados\n",
    "    \n",
    "    #train the model of selected features\n",
    "    selection_model = XGBRegressor(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, max_delta_step=0, \n",
    "                                    random_state=101, min_child_weight=1, missing=None, n_jobs=4, \n",
    "                                    scale_pos_weight=1, seed=None, silent=True, subsample=1, verbosity = 0)\n",
    "    selection_model.fit(select_X_train, y)\n",
    "    \n",
    "    #Evaluate the fitted model\n",
    "    select_X_test = selection.transform(X_test) #test set with selected features\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    predictions = [round(value) for value in y_pred] #redondeo los valores de la predicción\n",
    "    r2 = r2_score(y_test, predictions) #calculo el R2\n",
    "    mse = mean_squared_error(y_test, predictions) #calculo el MSE\n",
    "#     print(\"Thresh={:1.3f}, n={:d}, R2: {:2.2%} with MSE: {:.4f}\".format(thresh, select_X_train.shape[1], r2, mse))\n",
    "    \n",
    "    #select the best \n",
    "    if best >= mse:\n",
    "        best = mse\n",
    "        colsbest = select_X_train.shape[1]\n",
    "        my_model = selection_model\n",
    "        threshold = thresh\n",
    "        \n",
    "        \n",
    "#Present results\n",
    "fig = plt.figure(figsize = (20,20))\n",
    "ax = fig.add_subplot(131)\n",
    "g = plot_importance(my_model, height = 0.5, ax = ax, title='The best MSE: {:1.4f} with {:d} features'.\\\n",
    "                    format(best, colsbest))  \n",
    "plt.show()\n",
    "#Create a summary\n",
    "feature_importances = [(score, feature) for score, feature in zip(model.feature_importances_, cols)] #todas las feat importance\n",
    "XGBest = pd.DataFrame(sorted(feature_importances, reverse= True)[:colsbest], columns = [\"Score\", \"Feature\"])\n",
    "Xgb_selected_feats = XGBest[\"Feature\"].values\n",
    "print(\"Sumary:\")\n",
    "print(\"The best MSE was obtained with {} features\".format(colsbest))\n",
    "print(\"Selected features:\",Xgb_selected_feats )\n",
    "print(XGBest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNIMOS LOS FEATURES SELECCIONADOS POR TODOS LOS METODOS\n",
    "#Unidos todos los features seleccionados por los 5 métodos:\n",
    "bcols_mae = set(pv_cols).union(set(sel_cols_rfecv)).union(set(SBS)).union(set(kbest_FR)).union(set(kbest_MIR)).union(set(Xgb_selected_feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos qué features aportó cada método que no aportaron los otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionados diferenciales por kbest_FR:\n",
    "set(kbest_FR).difference(set(pv_cols).union(set(sel_cols_rfecv)).union(set(SBS)).union(set(kbest_MIR)).union(set(Xgb_selected_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Seleccionados diferenciales por kbest_MIR:\n",
    "set(kbest_MIR).difference(set(pv_cols).union(set(sel_cols_rfecv)).union(set(SBS)).union(set(kbest_FR)).union(set(Xgb_selected_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionados diferenciales por RFECV:\n",
    "set(sel_cols_rfecv).difference(set(pv_cols).union(set(SBS)).union(set(kbest_FR)).union(set(kbest_MIR)).union(set(Xgb_selected_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seleccionados diferenciales por SBS\n",
    "set(SBS).difference(set(pv_cols).union(set(sel_cols_rfecv)).union(set(kbest_FR)).union(set(kbest_MIR)).union(set(Xgb_selected_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionados diferenciales por xgboost...\n",
    "set(Xgb_selected_feats).difference(set(pv_cols).union(set(sel_cols_rfecv)).union(set(SBS)).union(set(kbest_FR)).union(set(kbest_MIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleccionados diferenciales por Recursive Feature mae (pv)\n",
    "set(pv_cols).difference(set(sel_cols_rfecv).union(set(SBS)).union(set(kbest_FR)).union(set(kbest_MIR)).union(set(Xgb_selected_feats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Union of all features selected total:\", len(bcols_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##COMO EL VIEJO COMETIO UN ERROR EN Kbest_MIR, VAMOS A USAR SU SELECCION ACA PARA EFECTOS DE REPRODUCIBILIDAD DE LO QUE VIENE\n",
    "bcols = {'Alley_NA',\n",
    " 'Alley_Pave',\n",
    " 'BedroomAbvGr',\n",
    " 'BsmtCond',\n",
    " 'BsmtExposure',\n",
    " 'BsmtFinType1',\n",
    " 'BsmtFinType2',\n",
    " 'BsmtFullBath',\n",
    " 'BsmtHalfBath',\n",
    " 'BsmtQual',\n",
    " 'CentralAir',\n",
    " 'Condition1_Artery',\n",
    " 'Condition1_Feedr',\n",
    " 'Condition1_Norm',\n",
    " 'Condition1_RRAe',\n",
    " 'Condition1_RRAn',\n",
    " 'Condition2_Artery',\n",
    " 'Condition2_Feedr',\n",
    " 'ConstructArea',\n",
    " 'Electrical_FuseA',\n",
    " 'Electrical_FuseF',\n",
    " 'EnclosedPorch',\n",
    " 'ExterCond',\n",
    " 'ExterQual',\n",
    " 'Exterior1st_AsbShng',\n",
    " 'Exterior1st_AsphShn',\n",
    " 'Exterior1st_BrkComm',\n",
    " 'Exterior1st_BrkFace',\n",
    " 'Exterior1st_CemntBd',\n",
    " 'Exterior1st_HdBoard',\n",
    " 'Exterior1st_Plywood',\n",
    " 'Exterior1st_Stucco',\n",
    " 'Exterior1st_WdSdng',\n",
    " 'Exterior1st_WdShing',\n",
    " 'Exterior2nd_AsbShng',\n",
    " 'Exterior2nd_BrkFace',\n",
    " 'Exterior2nd_CmentBd',\n",
    " 'Exterior2nd_HdBoard',\n",
    " 'Exterior2nd_MetalSd',\n",
    " 'Exterior2nd_Plywood',\n",
    " 'Exterior2nd_Stone',\n",
    " 'Exterior2nd_WdSdng',\n",
    " 'Fence',\n",
    " 'Fireplaces',\n",
    " 'Foundation_BrkTil',\n",
    " 'Foundation_PConc',\n",
    " 'Foundation_Slab',\n",
    " 'FullBath',\n",
    " 'Functional',\n",
    " 'GarageArea_x_Car',\n",
    " 'GarageFinish',\n",
    " 'GarageType_Basment',\n",
    " 'GarageType_BuiltIn',\n",
    " 'GarageType_CarPort',\n",
    " 'GarageType_Detchd',\n",
    " 'GarageType_NA',\n",
    " 'GarageYrBlt',\n",
    " 'Garage_Newest',\n",
    " 'HalfBath',\n",
    " 'HeatingQC',\n",
    " 'Heating_GasA',\n",
    " 'Heating_GasW',\n",
    " 'Heating_Grav',\n",
    " 'HouseStyle_15Fin',\n",
    " 'HouseStyle_15Unf',\n",
    " 'HouseStyle_SFoyer',\n",
    " 'HouseStyle_SLvl',\n",
    " 'IsNew',\n",
    " 'KitchenAbvGr',\n",
    " 'KitchenQual',\n",
    " 'LandContour_Lvl',\n",
    " 'LandSlope',\n",
    " 'LotAreaMultSlope',\n",
    " 'LotConfig_CulDSac',\n",
    " 'LotConfig_FR2',\n",
    " 'LotConfig_Inside',\n",
    " 'LotFrontage',\n",
    " 'LotShape',\n",
    " 'MSSubClass_120',\n",
    " 'MSSubClass_160',\n",
    " 'MSSubClass_180',\n",
    " 'MSSubClass_190',\n",
    " 'MSSubClass_20',\n",
    " 'MSSubClass_30',\n",
    " 'MSSubClass_40',\n",
    " 'MSSubClass_50',\n",
    " 'MSSubClass_60',\n",
    " 'MSSubClass_70',\n",
    " 'MSSubClass_75',\n",
    " 'MSSubClass_80',\n",
    " 'MSSubClass_85',\n",
    " 'MSZoning_FV',\n",
    " 'MSZoning_RM',\n",
    " 'MasVnrArea',\n",
    " 'MasVnrType_BrkFace',\n",
    " 'MasVnrType_Stone',\n",
    " 'MiscFeature_Othr',\n",
    " 'MiscFeature_Pool',\n",
    " 'MiscFeature_Shed',\n",
    " 'MoSold_1',\n",
    " 'MoSold_11',\n",
    " 'MoSold_12',\n",
    " 'MoSold_2',\n",
    " 'MoSold_3',\n",
    " 'MoSold_4',\n",
    " 'MoSold_5',\n",
    " 'MoSold_6',\n",
    " 'MoSold_7',\n",
    " 'MoSold_8',\n",
    " 'MoSold_9',\n",
    " 'Neighborhood_Blmngtn',\n",
    " 'Neighborhood_Blueste',\n",
    " 'Neighborhood_BrDale',\n",
    " 'Neighborhood_BrkSide',\n",
    " 'Neighborhood_ClearCr',\n",
    " 'Neighborhood_CollgCr',\n",
    " 'Neighborhood_Crawfor',\n",
    " 'Neighborhood_Edwards',\n",
    " 'Neighborhood_Gilbert',\n",
    " 'Neighborhood_IDOTRR',\n",
    " 'Neighborhood_MeadowV',\n",
    " 'Neighborhood_Mitchel',\n",
    " 'Neighborhood_NAmes',\n",
    " 'Neighborhood_NPkVill',\n",
    " 'Neighborhood_NoRidge',\n",
    " 'Neighborhood_NridgHt',\n",
    " 'Neighborhood_OldTown',\n",
    " 'Neighborhood_SWISU',\n",
    " 'Neighborhood_Sawyer',\n",
    " 'Neighborhood_SawyerW',\n",
    " 'Neighborhood_Somerst',\n",
    " 'Neighborhood_StoneBr',\n",
    " 'Neighborhood_Timber',\n",
    " 'Neighborhood_Veenker',\n",
    " 'OpenPorchSF',\n",
    " 'OverallCond',\n",
    " 'PavedDrive',\n",
    " 'Remod',\n",
    " 'RoofMatl_CompShg',\n",
    " 'RoofMatl_TarGrv',\n",
    " 'RoofMatl_WdShake',\n",
    " 'RoofMatl_WdShngl',\n",
    " 'RoofStyle_Gambrel',\n",
    " 'RoofStyle_Hip',\n",
    " 'RoofStyle_Mansard',\n",
    " 'RoofStyle_Shed',\n",
    " 'SaleCondition_Abnorml',\n",
    " 'SaleCondition_AdjLand',\n",
    " 'SaleCondition_Alloca',\n",
    " 'SaleCondition_Family',\n",
    " 'SaleType_ConLD',\n",
    " 'SaleType_WD',\n",
    " 'Street',\n",
    " 'TotBathrooms',\n",
    " 'TotRmsAbvGrd',\n",
    " 'TotalExtraPoints',\n",
    " 'TotalPoints',\n",
    " 'YearBuilt',\n",
    " 'YearRemodAdd',\n",
    " 'YrSold'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FINAL SUMMARY:\n",
    "print(\"Union of all features selected total:\", len(bcols))\n",
    "print(\"Features that will be left out from the original set if we use this final selection:\", cols.difference(bcols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r new_poly_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####ENSAMBLE DE FEATURES DESPUÉS DE FEATURE SELECTION\n",
    "\"\"\"Agregaremos los features bcols (que reemplaza ya al original cols) seleccionados a new_poly_features (que creamos antes)\n",
    "y formamos con eso\n",
    "el set de features actualizado en total_cols\"\"\"\n",
    "total_cols = list(bcols.union(set(new_poly_features.columns))) #unimos las columnas\n",
    "len(total_cols)\n",
    "total_cols\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"LotFrontage\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ACTUALIZACION DE TRAIN Y TEST ANTES DE PASAR A MODELING\n",
    "\n",
    "# #Creo el train y se resetea también su index\n",
    "train = all_data.round(3).iloc[:len(y_train), :]\n",
    "train = train.loc[:, list(total_cols)].reset_index(drop = True, inplace = False)\n",
    "\n",
    "# #Creo el test y se resetea también su index\n",
    "test = all_data.round(3).iloc[len(y_train):, :]\n",
    "test= test.loc[:, list(total_cols)].reset_index(drop = True, inplace = False)\n",
    "\n",
    "# #Reseteo además el y_train en su index\n",
    "\n",
    "y_train = y_train.reset_index(drop = True, inplace = False)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DIMENSIONALITY REDUCTION VIA PRINCIPAL COMPONENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Excelente explicación en este link: https://www.youtube.com/watch?v=AniiwysJ-2Y&list=PLs8w1Cdi-zvZ43xD_AA-eAuEW1FLK0cef&index=6\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Haremos una reducción dimensional de la data utilizando  PCA:\n",
    "#Escalamos los datos: es un requisito antes de aplicar PCA\n",
    "scale = RobustScaler() \n",
    "df = scale.fit_transform(train)\n",
    "\n",
    "pca = PCA().fit(df) # whiten=True\n",
    "\n",
    "print('With only 120 features: {:6.4%} of the variance was captured'.format(sum(pca.explained_variance_ratio_[:120])),\"\\n\")\n",
    "\n",
    "print('After PCA, {:3} features only left not explained {:6.4%} of variance ratio from the original {:3}'.format(120,\n",
    "                                                                                    (sum(pca.explained_variance_ratio_[120:])),\n",
    "                                                                                    df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca.explained_variance_ratio_[:120])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Nota: el número de features seleccionados fue elegido a criterio. Uno puede elegir seteando pca en n_components_ = m, lo que arrojará\n",
    "los m mejores componentes proyectados. En este caso se dejó m = 190 pero se cortó en 120 para mostrar que con 120 ya se tenía\n",
    "el 99.9% de la varianza explicada (es decir, se captura el 99.9% de la información de los datos originales)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class select_fetaures(object): # BaseEstimator, TransformerMixin, \n",
    "    def __init__(self, select_cols):\n",
    "        self.select_cols_ = select_cols\n",
    "\n",
    "    def fit(self, X, Y ):\n",
    "        print('Received {0:2d} features...'.format(X.shape[1]))\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        print('Select {0:2d} features'.format(X.loc[:, self.select_cols_].shape[1]))\n",
    "        return X.loc[:, self.select_cols_]    \n",
    "\n",
    "    def fit_transform(self, X, Y):\n",
    "        self.fit(X, Y)\n",
    "        df = self.transform(X)\n",
    "        return df \n",
    "        #X.loc[:, self.select_cols_]    \n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        return self.X[x], self.Y[x]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, name='NAN', log=False):\n",
    "    \n",
    "    rcols = ['Name','Model', 'BestParameters', 'Scorer', 'Index', 'BestScore', 'BestScoreStd', 'MeanScore', \n",
    "             'MeanScoreStd', 'Best']\n",
    "    res = pd.DataFrame(columns=rcols)\n",
    "    results = gs.cv_results_\n",
    "    modelo = gs.best_estimator_\n",
    "\n",
    "    scoring = {'MAE': 'neg_mean_absolute_error', 'R2': 'r2', 'RMSE': 'neg_mean_squared_error'}\n",
    "\n",
    "    for scorer in sorted(scoring):\n",
    "        best_index = np.nonzero(results['rank_test_%s' % scoring[scorer]] == 1)[0][0]\n",
    "        if scorer == 'RMSE': \n",
    "            best = np.sqrt(-results['mean_test_%s' % scoring[scorer]][best_index])\n",
    "            best_std = np.sqrt(results['std_test_%s' % scoring[scorer]][best_index])\n",
    "            scormean = np.sqrt(-results['mean_test_%s' % scoring[scorer]].mean())\n",
    "            stdmean = np.sqrt(results['std_test_%s' % scoring[scorer]].mean())\n",
    "            if log:\n",
    "                best = np.expm1(best)\n",
    "                best_std = np.expm1(best_std)\n",
    "                scormean = np.expm1(scormean)\n",
    "                stdmean = np.expm1(stdmean)\n",
    "        elif scorer == 'MEA':\n",
    "            best = (-results['mean_test_%s' % scoring[scorer]][best_index])\n",
    "            best_std = results['std_test_%s' % scoring[scorer]][best_index]\n",
    "            scormean =(-results['mean_test_%s' % scoring[scorer]].mean())\n",
    "            stdmean = results['std_test_%s' % scoring[scorer]].mean()\n",
    "            if log:\n",
    "                best = np.expm1(best)\n",
    "                best_std = np.expm1(best_std)\n",
    "                scormean = np.expm1(scormean)\n",
    "                stdmean = np.expm1(stdmean)\n",
    "        else:\n",
    "            best = results['mean_test_%s' % scoring[scorer]][best_index]*100\n",
    "            best_std = results['std_test_%s' % scoring[scorer]][best_index]*100\n",
    "            scormean = results['mean_test_%s' % scoring[scorer]].mean()*100\n",
    "            stdmean = results['std_test_%s' % scoring[scorer]].mean()*100\n",
    "        \n",
    "        r1 = pd.DataFrame([(name, modelo, gs.best_params_, scorer, best_index, best, best_std, scormean, \n",
    "                            stdmean, gs.best_score_)],\n",
    "                          columns = rcols)\n",
    "        res = res.append(r1)\n",
    "        \n",
    "    if log:\n",
    "        bestscore = np.expm1(np.sqrt(-gs.best_score_))\n",
    "    else:\n",
    "        bestscore = np.sqrt(-gs.best_score_)\n",
    "        \n",
    "    print(\"Best Score: {:.6f}\".format(bestscore))\n",
    "    print('---------------------------------------')\n",
    "    print('Best Parameters:')\n",
    "    print(gs.best_params_)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "        ('pca', PCA(random_state = 101)),\n",
    "        ('model', Lasso(random_state = 101))]) \n",
    "\n",
    "SEL = list(set(sel_cols_rfecv).union(set(new_poly_features.columns)))\n",
    "n_components = [len(SEL)-5, len(SEL)-3, len(SEL)] \n",
    "whiten = [False, True]\n",
    "max_iter = [5] #, 10, 100, 200, 300, 400, 500, 600]  \n",
    "alpha = [0.0003, 0.0007, 0.0005, 0.05, 0.5, 1.0]\n",
    "selection = ['random', 'cyclic'] \n",
    "tol = [2e-03, 0.003, 0.001, 0.0005]\n",
    "param_grid =\\\n",
    "            dict(\n",
    "                  model__alpha = alpha\n",
    "                  ,model__max_iter = max_iter\n",
    "                  ,model__selection = selection\n",
    "                  ,model__tol = tol\n",
    "                  ,pca__n_components = n_components\n",
    "                  ,pca__whiten = whiten \n",
    "                ) \n",
    "\n",
    "gs = GridSearchCV(estimator = model, param_grid = param_grid, refit = 'neg_mean_squared_error' #, iid=False\n",
    "                   , scoring=list(['neg_mean_squared_error' , 'neg_mean_absolute_error', 'r2']) \n",
    "                   ,cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "lasso = Pipeline([\n",
    "        ('sel', select_fetaures(select_cols=SEL)), \n",
    "        ('scl', RobustScaler()),\n",
    "        ('gs', gs)\n",
    " ])\n",
    "\n",
    "lasso.fit(train,y_train)\n",
    "\n",
    "results = get_results(lasso, 'lasso Lg1', log=True)\n",
    "display(results.loc[:, 'Scorer' : 'MeanScoreStd'])\n",
    "r = residuals_plots(lasso, train, y_train, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
